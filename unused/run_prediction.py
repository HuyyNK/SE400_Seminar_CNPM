import joblib
from pathlib import Path
# Import l·ªõp HybridToxicClassifier t·ª´ file hybrid_classifier.py
from hybrid_classifier import HybridToxicClassifier
# Import l·ªõp ToxicPhraseDetector ƒë·ªÉ x·ª≠ l√Ω m·ªôt l·ªói nh·ªè khi t·∫£i m√¥ h√¨nh
# L·ªói n√†y ƒë√¥i khi x·∫£y ra v√¨ file .pkl c≈©ng tham chi·∫øu ƒë·∫øn l·ªõp n√†y
try:
    from CrawlData.model import ToxicPhraseDetector
except ImportError:
    # N·∫øu kh√¥ng t√¨m th·∫•y, t·∫°o m·ªôt l·ªõp gi·∫£ ƒë·ªÉ tr√°nh l·ªói khi t·∫£i
    class ToxicPhraseDetector:
        pass

def load_model():
    """
    T·∫£i m√¥ h√¨nh hybrid t·ª´ file.
    H√†m n√†y ƒë∆∞·ª£c t√°ch ri√™ng ƒë·ªÉ ch·ªâ t·∫£i m√¥ h√¨nh m·ªôt l·∫ßn.
    """
    try:
        model_path = Path(__file__).parent / 'saved_models' / 'hybrid_classifier_optimized.pkl'
        print("ƒêang t·∫£i m√¥ h√¨nh...")
        classifier = joblib.load(model_path)
        print("‚úì M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")
        return classifier
    except FileNotFoundError:
        print(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh t·∫°i '{model_path}'.")
        print("Vui l√≤ng ƒë·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y notebook 'toxic_classification_nb_hybrid.ipynb' ƒë·ªÉ t·∫°o file m√¥ h√¨nh.")
        return None
    except Exception as e:
        print(f"ƒê√£ x·∫£y ra l·ªói khi t·∫£i m√¥ h√¨nh: {e}")
        return None

def predict_toxicity(classifier, text: str):
    """
    S·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i ƒë·ªÉ d·ª± ƒëo√°n nh√£n cho m·ªôt vƒÉn b·∫£n.
    """
    if not text:
        return

    # Th·ª±c hi·ªán d·ª± ƒëo√°n
    result = classifier.predict(text)
    
    # In k·∫øt qu·∫£ chi ti·∫øt
    print("\n" + "="*50)
    print(f"K·∫øt qu·∫£ ph√¢n t√≠ch:")
    print(f"  VƒÉn b·∫£n: '{text}'")
    print("-" * 50)
    
    if result['label'] == 'VIOLATION':
        print(f"  => Ph√¢n lo·∫°i: VIOLATION üî¥")
    else:
        print(f"  => Ph√¢n lo·∫°i: SAFE üü¢")
        
    print(f"  Ph∆∞∆°ng ph√°p ph√°t hi·ªán: {result['method']}")
    
    if result['method'] == 'ml_model':
        prob = result.get('ml_probability', 0)
        print(f"  X√°c su·∫•t vi ph·∫°m (ML): {prob:.2%}")
    
    if result.get('toxic_phrases'):
        print(f"  C√°c t·ª´ vi ph·∫°m ph√°t hi·ªán (Lu·∫≠t): {result.get('toxic_phrases')}")
    print("="*50 + "\n")

# --- V√≤ng l·∫∑p ch√≠nh ƒë·ªÉ ng∆∞·ªùi d√πng nh·∫≠p li·ªáu ---
if __name__ == "__main__":
    # T·∫£i m√¥ h√¨nh ngay khi ch∆∞∆°ng tr√¨nh b·∫Øt ƒë·∫ßu
    hybrid_classifier = load_model()

    # Ch·ªâ ti·∫øp t·ª•c n·∫øu m√¥ h√¨nh ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng
    if hybrid_classifier:
        print("\nCh√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi tr√¨nh nh·∫≠n di·ªán n·ªôi dung ƒë·ªôc h·∫°i.")
        print("Nh·∫≠p m·ªôt c√¢u b·∫•t k·ª≥ ƒë·ªÉ ki·ªÉm tra.")
        print("G√µ 'quit' ho·∫∑c 'exit' ƒë·ªÉ tho√°t ch∆∞∆°ng tr√¨nh.\n")
        
        while True:
            # Y√™u c·∫ßu ng∆∞·ªùi d√πng nh·∫≠p m·ªôt c√¢u
            user_input = input("Nh·∫≠p c√¢u c·ªßa b·∫°n: ")
            
            # Ki·ªÉm tra ƒëi·ªÅu ki·ªán tho√°t
            if user_input.lower() in ['quit', 'exit']:
                print("T·∫°m bi·ªát!")
                break
            
            # Th·ª±c hi·ªán d·ª± ƒëo√°n v·ªõi c√¢u ng∆∞·ªùi d√πng nh·∫≠p
            predict_toxicity(hybrid_classifier, user_input)