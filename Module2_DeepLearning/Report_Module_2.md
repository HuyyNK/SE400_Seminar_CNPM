# BÃO CÃO MODULE 2 - PHÃ‚N LOáº I TOXIC COMMENTS
## Sá»­ dá»¥ng Deep Learning: CNN + BiLSTM

## 1. Tá»”NG QUAN

**Má»¥c tiÃªu**: PhÃ¢n loáº¡i bÃ¬nh luáº­n Ä‘á»™c háº¡i vá»›i 6 nhÃ£n: toxic, severe_toxic, obscene, threat, insult, identity_hate

**Kiáº¿n trÃºc chÃ­nh**: 
- â­ **Bidirectional LSTM** (3 layers: 128â†’64â†’32 units) - Xá»­ lÃ½ ngá»¯ cáº£nh tá»« 2 chiá»u
- â­ **Convolutional Neural Network** (Multi-kernel 3,4,5 vá»›i 128 filters) - Báº¯t character patterns
- Gated Fusion - Káº¿t há»£p adaptive giá»¯a 2 nhÃ¡nh

**Tech Stack**:
- TensorFlow 2.17.0 + Keras
- FastAPI 0.104.1  
- GloVe 300D embeddings
- Pydantic v2

**Dataset**: Kaggle Toxic Comment Classification (159K train, 40K val)

## 2. KIáº¾N TRÃšC MÃ” HÃŒNH: CNN + BiLSTM HYBRID

### 2.1. Tá»•ng quan
**Hybrid Deep Learning** káº¿t há»£p sá»©c máº¡nh cá»§a CNN vÃ  BiLSTM:

```
Input Text
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Word Branch: BiLSTM (Contextual Understanding)          â”‚
â”‚ GloVe 300D â†’ Stacked BiLSTM (128â†’64â†’32)                â”‚
â”‚           â†’ Multi-Head Attention (4 heads) â†’ Pooling    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Char Branch: CNN (Pattern Recognition)                  â”‚
â”‚ Embed 48D â†’ Residual CNN (kernels 3,4,5, 128 filters)  â”‚
â”‚          â†’ MaxPool â†’ Dense 128                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Gated Fusion: Adaptive weighting (gate Ã— word + (1-gate) Ã— char)
    â†“
Dense 128 â†’ Dropout 0.5 â†’ Dense 64 â†’ Dropout 0.5 â†’ Output (6 labels)
```

**Key specs**: Vocab 50K words/100 chars, Seq length: 150 tokens/200 chars, Pre-trained GloVe 300D (frozen)

### 2.2. Vai trÃ² cá»§a CNN vÃ  BiLSTM

**ğŸ”¹ Bidirectional LSTM (3 layers stacked)**:
- Xá»­ lÃ½ sequence tá»« 2 chiá»u (forward + backward)
- Layer 1 (128 units): Báº¯t word patterns
- Layer 2 (64 units): Báº¯t phrase patterns  
- Layer 3 (32 units): Báº¯t sentence-level context
- **Æ¯u Ä‘iá»ƒm**: Hiá»ƒu ngá»¯ cáº£nh toÃ n cá»¥c, phÃ¡t hiá»‡n toxic dá»±a vÃ o context

**ğŸ”¹ Convolutional Neural Network (Residual, Multi-kernel)**:
- 3 kernels song song (size 3, 4, 5) vá»›i 128 filters má»—i kernel
- Kernel 3: Báº¯t trigrams ("wtf", "f*k")
- Kernel 4: Báº¯t 4-grams ("hate", "damn")
- Kernel 5: Báº¯t 5-grams ("idiot", "moron")
- **Æ¯u Ä‘iá»ƒm**: PhÃ¡t hiá»‡n obfuscated text ("f_u_c_k", "l33tspeak")

**ğŸ”¹ Gated Fusion**: Káº¿t há»£p thÃ´ng minh giá»¯a BiLSTM vÃ  CNN
- "Great work!" â†’ gate â‰ˆ 0.9 (tin BiLSTM - clean text)
- "f_u_c_k y0u" â†’ gate â‰ˆ 0.3 (tin CNN - obfuscated)

**ğŸ”¹ Multi-Head Attention**: 4 heads Ã— 2 layers
- Táº­p trung vÃ o toxic keywords + surrounding context

## 3. TRAINING

**Preprocessing Pipeline**:
1. **Text Cleaning**:
   - Lowercase conversion
   - Remove URLs, mentions (@user), HTML tags
   - Whitespace normalization
   
2. **Advanced Normalization**:
   - **Obfuscated profanity**: "f*ck" â†’ "fuck", "sh1t" â†’ "shit", "b!tch" â†’ "bitch"
   - **Leet speak**: "@sshole" â†’ "asshole", "idi0t" â†’ "idiot", "sh1t" â†’ "shit"
   - **Character repetition**: "shiiiit" â†’ "shit", "fuuuuck" â†’ "fuck"
   - **Chat lingo**: "u" â†’ "you", "ur" â†’ "your", "wtf" â†’ "what the fuck"
   - **Punctuation collapse**: "!!!" â†’ "!", "???" â†’ "?"
   - **Emoji sentiment**: ğŸ˜  â†’ "angry", ğŸ˜€ â†’ "happy"
   
3. **Context-Aware Profanity**:
   - "fucking good" â†’ "very good" (benign context)
   - "fucking dead" â†’ giá»¯ nguyÃªn (toxic context)
   - "killer at chess" â†’ "expert at chess" (skill context)

4. **Tokenization**:
   - Word-level: 50K vocab, 150 max tokens
   - Char-level: 100 vocab, 200 max chars

**Loss Function**: Binary Focal Loss (Î³=2.0, Î±=0.25)
- Xá»­ lÃ½ extreme imbalance (threat: 0.3%, identity_hate: 0.08%)
- Focus vÃ o hard examples

**Optimization**:
- Adam optimizer (lr=0.001, clipnorm=1.0)
- Batch size: 512
- Regularization: Dropout 0.2-0.5, L2 reg 0.01, Label smoothing 0.1
- Callbacks: EarlyStopping (patience=5), ReduceLR (factor=0.5)
- tf.data pipeline: cache + shuffle + prefetch â†’ 2-3x faster

**Training result**: 18 epochs (early stopped), ~45 phÃºt GPU RTX 3060

## 4. Káº¾T QUáº¢

### 4.1. So sÃ¡nh Static vs Optimized Thresholds

#### Static Thresholds (0.5 for all labels)
| Label          | Precision | Recall | F1-Score | ROC-AUC | Threshold |
|----------------|-----------|--------|----------|---------|-----------|
| toxic          | 0.883     | 0.451  | 0.597    | 0.933   | 0.50      |
| severe_toxic   | 0.000     | 0.000  | 0.000    | 0.956   | 0.50      |
| obscene        | 0.678     | 0.224  | 0.337    | 0.913   | 0.50      |
| threat         | 0.000     | 0.000  | 0.000    | 0.868   | 0.50      |
| insult         | 0.716     | 0.102  | 0.179    | 0.901   | 0.50      |
| identity_hate  | 0.000     | 0.000  | 0.000    | 0.913   | 0.50      |
| **Macro Avg**  | **0.379** | **0.130** | **0.185** | **0.914** | -     |

#### Optimized Thresholds
| Label          | Precision | Recall | F1-Score | ROC-AUC | Threshold |
|----------------|-----------|--------|----------|---------|-----------|
| toxic          | 0.760     | 0.773  | **0.766** | 0.933  | **0.20**  |
| severe_toxic   | 0.248     | 0.308  | **0.275** | 0.956  | **0.10**  |
| obscene        | 0.462     | 0.567  | **0.509** | 0.913  | **0.30**  |
| threat         | 0.000     | 0.000  | 0.000    | 0.868  | 0.50      |
| insult         | 0.399     | 0.473  | **0.433** | 0.901  | **0.30**  |
| identity_hate  | 0.106     | 0.288  | **0.155** | 0.913  | **0.10**  |
| **Macro Avg**  | **0.329** | **0.402** | **0.356** | **0.914** | -     |

**Improvement**: F1 +92% (0.185 â†’ 0.356), Recall +207% (13% â†’ 40%)

### 4.2. Optimal Thresholds

| toxic | 0.20 | severe_toxic | 0.10 | obscene | 0.30 |
| threat | 0.50 | insult | 0.30 | identity_hate | 0.10 |

**Strategy**: Lower thresholds cho rare labels Ä‘á»ƒ maximize recall

### 4.3. Performance
- **Training**: 18 epochs, 45 min (GPU RTX 3060)
- **Inference**: 0.66ms/sample (batch mode), ~1,500 predictions/sec
- **Model size**: 73 MB (+ 60 MB embeddings cache)

## 5. API ENDPOINTS

**POST /predict** - Single text classification
```json
Input: {"text": "You are stupid!"}
Output: {
  "text": "...",
  "predictions": {"toxic": {"probability": 0.85, "predicted": true}, ...},
  "toxic_labels": ["toxic", "insult"],
  "is_toxic": true,
  "risk_level": "high"
}
```

**POST /predict/batch** - Batch classification (simplified)
```json
Input: {"texts": ["Great!", "You idiot!"]}
Output: [
  {"text": "Great!", "is_toxic": false},
  {"text": "You idiot!", "is_toxic": true}
]
```

**GET /health** - Health check
```json
Output: {"status": "healthy", "model_loaded": true}
```

## 6. Cáº¤U TRÃšC PROJECT

```
Module2_DL/
â”œâ”€â”€ app.py                          # API server chÃ­nh (FastAPI)
â”œâ”€â”€ requirements.txt                # Danh sÃ¡ch thÆ° viá»‡n cáº§n cÃ i
â”‚
â”œâ”€â”€ src/                            # MÃ£ nguá»“n chÃ­nh
â”‚   â”œâ”€â”€ core.py                     # Kiáº¿n trÃºc CNN + BiLSTM
â”‚   â”œâ”€â”€ preprocessing.py            # Tiá»n xá»­ lÃ½ text
â”‚   â”œâ”€â”€ processing.py               # Háº­u xá»­ lÃ½ káº¿t quáº£
â”‚   â””â”€â”€ utils.py                    # Háº±ng sá»‘, helper functions
â”‚
â”œâ”€â”€ models/                         # Scripts huáº¥n luyá»‡n
â”‚   â”œâ”€â”€ train.py                    # Script train model CNN + BiLSTM
â”‚   â””â”€â”€ optimize_thresholds.py      # Tá»‘i Æ°u ngÆ°á»¡ng phÃ¢n loáº¡i
â”‚
â”œâ”€â”€ artifacts/                      # Artifacts Ä‘Ã£ train
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ best_model.h5           # Model Ä‘Ã£ train (73MB)
â”‚   â”œâ”€â”€ embedding_matrix.npy        # GloVe embeddings cache (60MB)
â”‚   â”œâ”€â”€ tokenizer.json              # Word tokenizer
â”‚   â”œâ”€â”€ config.json                 # Cáº¥u hÃ¬nh training
â”‚   â””â”€â”€ reports/                    # Reports Ä‘Ã¡nh giÃ¡
â”‚       â”œâ”€â”€ training_history.json   # Lá»‹ch sá»­ training
â”‚       â”œâ”€â”€ evaluation_results.json # Káº¿t quáº£ Ä‘Ã¡nh giÃ¡
â”‚       â””â”€â”€ optimized_thresholds.json # NgÆ°á»¡ng tá»‘i Æ°u
â”‚
â””â”€â”€ embeddings/
    â””â”€â”€ glove.6B.300d.txt           # Pre-trained GloVe (822MB)
```

## 7. HÆ¯á»šNG DáºªN CHáº Y API

### BÆ°á»›c 1: CÃ i Ä‘áº·t mÃ´i trÆ°á»ng
```bash
# Clone repository
git clone https://github.com/HuyyNK/SE400_Seminar_CNPM.git
cd SE400_Seminar_CNPM/Module2_DL

# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python -m venv venv

# KÃ­ch hoáº¡t venv
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### BÆ°á»›c 2: Khá»Ÿi Ä‘á»™ng API
```bash
# Cháº¡y API server
python app.py

# Server sáº½ cháº¡y táº¡i: http://127.0.0.1:8000
# API Docs (Swagger): http://127.0.0.1:8000/docs
```

### BÆ°á»›c 3: Test API

**CÃ¡ch 1: Sá»­ dá»¥ng Swagger UI**
- Má»Ÿ trÃ¬nh duyá»‡t: http://127.0.0.1:8000/docs
- Click "Try it out" â†’ Nháº­p text â†’ "Execute"

**CÃ¡ch 2: Sá»­ dá»¥ng curl**
```bash
# Single prediction
curl -X POST "http://127.0.0.1:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"text": "You are stupid!"}'

# Batch prediction
curl -X POST "http://127.0.0.1:8000/predict/batch" \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Great work!", "You idiot!"]}'
```

**CÃ¡ch 3: Sá»­ dá»¥ng Python**
```python
import requests

# Single prediction
response = requests.post(
    "http://127.0.0.1:8000/predict",
    json={"text": "You are stupid!"}
)
print(response.json())

# Batch prediction
response = requests.post(
    "http://127.0.0.1:8000/predict/batch",
    json={"texts": ["Great!", "You idiot!"]}
)
print(response.json())
```

### LÆ°u Ã½:
- Láº§n Ä‘áº§u cháº¡y máº¥t ~3-5 giÃ¢y Ä‘á»ƒ load model
- YÃªu cáº§u RAM tá»‘i thiá»ƒu: 4GB
- GPU khÃ´ng báº¯t buá»™c (CPU Ä‘á»§ nhanh cho inference)

---

## 8. Káº¾T LUáº¬N

Module 2 xÃ¢y dá»±ng thÃ nh cÃ´ng há»‡ thá»‘ng phÃ¢n loáº¡i toxic comments sá»­ dá»¥ng **Deep Learning vá»›i CNN vÃ  BiLSTM**:

**Kiáº¿n trÃºc**:
- â­ **Bidirectional LSTM** (3 layers: 128â†’64â†’32 units): Xá»­ lÃ½ ngá»¯ cáº£nh 2 chiá»u
- â­ **Convolutional Neural Network** (128 filters, kernels 3,4,5): Báº¯t character patterns
- Gated Fusion: Káº¿t há»£p adaptive giá»¯a BiLSTM vÃ  CNN
- Multi-Head Attention (4 heads Ã— 2 layers): Focus vÃ o toxic keywords

**Káº¿t quáº£**:
- Macro F1-score: **0.356** vá»›i optimized thresholds (+92% vs baseline 0.185)
- Recall: **40%** (tÄƒng 3x so vá»›i static threshold 13%)
- Inference time: **0.66ms/sample** (1,500 predictions/sec)

**á»¨ng dá»¥ng**: API FastAPI production-ready, cÃ³ thá»ƒ tÃ­ch há»£p vÃ o social media, forums Ä‘á»ƒ tá»± Ä‘á»™ng kiá»ƒm duyá»‡t ná»™i dung Ä‘á»™c háº¡i.

