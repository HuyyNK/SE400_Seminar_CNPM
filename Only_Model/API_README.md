# Toxic Comment Classification API

REST API Ä‘á»ƒ phÃ¡t hiá»‡n comment Ä‘á»™c háº¡i (toxic) real-time sá»­ dá»¥ng TF-IDF + Logistic Regression.

## ğŸš€ TÃ­nh NÄƒng

- âœ… **Single Prediction**: PhÃ¢n tÃ­ch 1 comment
- âœ… **Batch Prediction**: PhÃ¢n tÃ­ch nhiá»u comments cÃ¹ng lÃºc (tá»‘i Ä‘a 100)
- âœ… **Context-Aware**: Nháº­n diá»‡n profanity trong ngá»¯ cáº£nh tÃ­ch cá»±c
- âœ… **Multi-label**: 6 nhÃ£n (toxic, severe_toxic, obscene, threat, insult, identity_hate)
- âœ… **Fast**: < 100ms cho 1 prediction
- âœ… **CORS Enabled**: Há»— trá»£ frontend integration

## ğŸ“‹ YÃªu Cáº§u

- Python 3.8+
- Trained models (tá»« `test_notebook.ipynb`)

## ğŸ”§ CÃ i Äáº·t

### 1. Clone/Navigate to project

```bash
cd d:\SE405_SE400\SE400_Seminar_CNPM\Only_Model
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Train vÃ  Save Models

Má»Ÿ `test_notebook.ipynb` vÃ  cháº¡y cÃ¡c cells Ä‘á»ƒ train model, sau Ä‘Ã³:

```python
# Trong Jupyter notebook, sau khi train xong (cell 16)
exec(open('save_models.py').read())
```

Hoáº·c export models báº±ng code trong notebook:

```python
import pickle
from pathlib import Path

# Create models directory
Path("models").mkdir(exist_ok=True)

# Save models
with open('models/lr_models.pkl', 'wb') as f:
    pickle.dump(lr_models, f)
    
with open('models/tfidf_word.pkl', 'wb') as f:
    pickle.dump(tfidf_word, f)
    
with open('models/tfidf_char.pkl', 'wb') as f:
    pickle.dump(tfidf_char, f)

print("âœ“ Models saved!")
```

### 4. Start API

```bash
python app.py
```

API sáº½ cháº¡y táº¡i: `http://localhost:5000`

## ğŸ“š API Endpoints

### 1. Health Check

```bash
GET /health
```

Response:
```json
{
  "status": "healthy",
  "models_loaded": true,
  "vectorizers_loaded": true
}
```

### 2. Single Prediction

```bash
POST /predict
Content-Type: application/json

{
  "text": "Your comment here",
  "threshold": 0.5
}
```

Response:
```json
{
  "text": "Your comment here",
  "normalized_text": "your comment here",
  "predictions": {
    "toxic": 0.123,
    "severe_toxic": 0.045,
    "obscene": 0.067,
    "threat": 0.012,
    "insult": 0.089,
    "identity_hate": 0.008
  },
  "is_toxic": false,
  "toxic_labels": [],
  "max_toxicity": {
    "label": "toxic",
    "score": 0.123
  },
  "threshold": 0.5
}
```

### 3. Batch Prediction

```bash
POST /batch
Content-Type: application/json

{
  "texts": ["comment 1", "comment 2", "..."],
  "threshold": 0.5
}
```

Response:
```json
{
  "results": [
    {
      "text": "comment 1",
      "normalized_text": "comment 1",
      "predictions": {...},
      "is_toxic": false,
      "toxic_labels": [],
      "max_toxicity": {...}
    },
    ...
  ],
  "summary": {
    "total": 10,
    "toxic": 3,
    "clean": 7,
    "toxic_percentage": 30.0
  },
  "threshold": 0.5
}
```

## ğŸ§ª Testing

```bash
# Test API
python test_api.py
```

Hoáº·c dÃ¹ng curl:

```bash
# Health check
curl http://localhost:5000/health

# Single prediction
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "You are an idiot"}'

# Batch prediction
curl -X POST http://localhost:5000/batch \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Great work!", "You suck!"]}'
```

## ğŸ’» Frontend Integration

### JavaScript/React Example

```javascript
// Single prediction
async function analyzeToxicity(text) {
  const response = await fetch('http://localhost:5000/predict', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ text, threshold: 0.5 }),
  });
  
  const result = await response.json();
  return result;
}

// Usage
const result = await analyzeToxicity("Your comment here");
console.log(result.is_toxic); // true/false
console.log(result.toxic_labels); // ["toxic", "insult"]
```

### Python Client Example

```python
import requests

# Single prediction
response = requests.post(
    'http://localhost:5000/predict',
    json={'text': 'Your comment here', 'threshold': 0.5}
)

result = response.json()
print(f"Is toxic: {result['is_toxic']}")
print(f"Labels: {result['toxic_labels']}")
```

## ğŸ¯ Use Cases

1. **Social Media Moderation**: Tá»± Ä‘á»™ng phÃ¡t hiá»‡n bÃ¬nh luáº­n Ä‘á»™c háº¡i
2. **Forum/Community**: Lá»c ná»™i dung trÆ°á»›c khi Ä‘Äƒng
3. **Customer Support**: Cáº£nh bÃ¡o tin nháº¯n khÃ´ng phÃ¹ há»£p
4. **Content Filtering**: PhÃ¢n loáº¡i ná»™i dung UGC
5. **Real-time Chat**: Kiá»ƒm soÃ¡t chat Ä‘á»™c háº¡i

## âš™ï¸ Configuration

### Thay Ä‘á»•i threshold

Máº·c Ä‘á»‹nh: `0.5`. Giáº£m Ä‘á»ƒ detect nhiá»u hÆ¡n (cÃ³ thá»ƒ false positives), tÄƒng Ä‘á»ƒ cháº·t cháº½ hÆ¡n.

```json
{
  "text": "...",
  "threshold": 0.3  // Cháº·t hÆ¡n
}
```

### Port Configuration

Äá»•i port trong `app.py`:

```python
app.run(host='0.0.0.0', port=8080, debug=False)
```

### Production Deployment

DÃ¹ng production server (khÃ´ng dÃ¹ng Flask development server):

```bash
pip install gunicorn

# Run with gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

## ğŸ“Š Performance

- **Inference Time**: ~50-100ms per prediction
- **Throughput**: ~500-1000 requests/second (vá»›i gunicorn multi-worker)
- **Memory**: ~200MB (models loaded)
- **Model Size**: ~50MB total

## ğŸ”’ Security Notes

- âš ï¸ **CORS**: Máº·c Ä‘á»‹nh enable táº¥t cáº£ origins. Production nÃªn giá»›i háº¡n:
  
  ```python
  CORS(app, origins=["https://yourdomain.com"])
  ```

- âš ï¸ **Rate Limiting**: ThÃªm rate limiter cho production:
  
  ```bash
  pip install flask-limiter
  ```

- âš ï¸ **Input Validation**: API Ä‘Ã£ cÃ³ basic validation, nhÆ°ng nÃªn thÃªm sanitization cho production

## ğŸ› Troubleshooting

### Models not found

```
âŒ Error: Model files not found!
```

**Solution**: Run `save_models.py` tá»« notebook sau khi train model.

### NLTK data missing

```
LookupError: Resource punkt not found
```

**Solution**:
```python
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
```

### Port already in use

```
OSError: [Errno 48] Address already in use
```

**Solution**: Äá»•i port hoáº·c kill process Ä‘ang dÃ¹ng port 5000:

```bash
# Windows
netstat -ano | findstr :5000
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:5000 | xargs kill -9
```

## ğŸ“– Model Details

- **Algorithm**: TF-IDF + Logistic Regression
- **Features**: 
  - Word n-grams (1-3): 80,000 features
  - Char n-grams (3-5): 20,000 features
- **Preprocessing**:
  - Profanity normalization
  - Context-aware profanity detection
  - Leet speak normalization (@ â†’ a)
  - Chat lingo expansion (u â†’ you)
- **Training Data**: Jigsaw Toxic Comment Classification (~159k comments)

## ğŸ“ License

MIT License

## ğŸ‘¥ Contributors

- Your Name

## ğŸ”— Links

- Dataset: [Jigsaw Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)
- Notebook: `test_notebook.ipynb`
