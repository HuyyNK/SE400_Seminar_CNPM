{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a92573",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification Pipeline\n",
    "\n",
    "## Dataset: Jigsaw Toxic Comment Classification\n",
    "- **Source**: Wikipedia talk pages (~159k comments)\n",
    "- **Task**: Multi-label classification với 6 nhãn phụ: toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "- **Challenge**: Dữ liệu mất cân bằng (class 0 ~89.8%, class 1 ~10.2%)\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **EDA & Preprocessing**: Khám phá, tiền xử lý với profanity normalization\n",
    "2. **TF-IDF + Logistic Regression**: Model với tokenization/lemmatization\n",
    "3. **Evaluation**: Accuracy, F1, ROC-AUC, PR-AUC cho từng nhãn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82fdf2f",
   "metadata": {},
   "source": [
    "# 1. Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16d4d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn - ML models and utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    f1_score, roc_auc_score, average_precision_score,\n",
    "    accuracy_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# NLTK for tokenization and lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data (uncomment if needed)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Scipy for sparse matrix operations\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2ff55",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4ca2f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (159571, 8)\n",
      "\n",
      "Columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../Data/train.csv')\n",
    "\n",
    "# Define label columns\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f94c7",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3d616f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "id               0\n",
      "comment_text     0\n",
      "toxic            0\n",
      "severe_toxic     0\n",
      "obscene          0\n",
      "threat           0\n",
      "insult           0\n",
      "identity_hate    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate rows: 0\n",
      "\n",
      "Total comments: 159571\n",
      "Unique comments: 159571\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nTotal comments: {len(df)}\")\n",
    "print(f\"Unique comments: {df['comment_text'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b8139c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "============================================================\n",
      "toxic               :  15294 ( 9.58%)\n",
      "severe_toxic        :   1595 ( 1.00%)\n",
      "obscene             :   8449 ( 5.29%)\n",
      "threat              :    478 ( 0.30%)\n",
      "insult              :   7877 ( 4.94%)\n",
      "identity_hate       :   1405 ( 0.88%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Any toxic label     :  16225 (10.17%)\n",
      "Clean comments      : 143346 (89.83%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze label distribution\n",
    "print(\"Label Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "for col in label_cols:\n",
    "    count = df[col].sum()\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"{col:20s}: {count:6d} ({pct:5.2f}%)\")\n",
    "\n",
    "# Calculate how many comments have at least one toxic label\n",
    "df['any_toxic'] = (df[label_cols].sum(axis=1) > 0).astype(int)\n",
    "toxic_count = df['any_toxic'].sum()\n",
    "toxic_pct = (toxic_count / len(df)) * 100\n",
    "\n",
    "print(f\"\\n{'Any toxic label':20s}: {toxic_count:6d} ({toxic_pct:5.2f}%)\")\n",
    "print(f\"{'Clean comments':20s}: {len(df) - toxic_count:6d} ({100-toxic_pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63ccb059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample comments:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[CLEAN COMMENT]\n",
      "\"\n",
      "\n",
      "Oh, don't worry about me, Sandstein. I'm of no strong opinion as to what is \"\"well.\"\" Editing Wikipedia is not a personal benefit; if it were, I'd be COI! I do have some unfinished business, both...\n",
      "\n",
      "[TOXIC COMMENT]\n",
      "\"\n",
      "There are no personal attacks. Just me pointing out that you are a really lousy judger of reliable sources. If you think that a source which claims Evanescence is gothic rock is reliable enough to ...\n",
      "\n",
      "[SEVERE_TOXIC COMMENT]\n",
      "98.248.32.178 I will set you on fire, I will shoot your ass up. I will cut your penis off and I will shove it down your throat and choke you. I will cut you up big time motherfucker.\n",
      "\n",
      "[OBSCENE COMMENT]\n",
      "why arr you so fuck \n",
      "\n",
      "why are you so fucking shit 182.16.240.42\n",
      "\n",
      "[THREAT COMMENT]\n",
      "Personal Attack Number 2 \n",
      "\n",
      "This is another personal attack about you being a massive donkey dick sucking homosexual. This is a concerned plea that you should at once drown yourself in a sewer. Fucki...\n",
      "\n",
      "[INSULT COMMENT]\n",
      "FUCK YOU \n",
      "\n",
      "YOU SAD PIECE OF SHIT\n",
      "GET A LIFE\n",
      "DONT THREATEN ME WITH BLOCKING ME OFF THIS SHIT\n",
      "DO I GIVE A FUCK\n",
      "YOU LONELY HERMIT MAN\n",
      "OPEN YOUR CURTAINS AND GET A LIFE BEFORE I BURN YOU ALIVE\n",
      "\n",
      "[IDENTITY_HATE COMMENT]\n",
      "\"\n",
      "\n",
      " TYPICAL COMMUNIST CENSORSHIP. WIKIPEDIA SHOULD NOT LET TRANNY PSYCHOPATHS CONTROL THEIR ARTICLES! THESE PEOPLE SHOULD BE LOCKED AWAY IN A NUT HOUSE! \n",
      "\n",
      "This is why Wikipedia is a joke and can n...\n"
     ]
    }
   ],
   "source": [
    "# Show sample comments from each category\n",
    "print(\"Sample comments:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean comment\n",
    "print(\"\\n[CLEAN COMMENT]\")\n",
    "clean_sample = df[df['any_toxic'] == 0].sample(1)['comment_text'].values[0]\n",
    "print(clean_sample[:200] + \"...\" if len(clean_sample) > 200 else clean_sample)\n",
    "\n",
    "# Toxic comments for each label\n",
    "for label in label_cols:\n",
    "    print(f\"\\n[{label.upper()} COMMENT]\")\n",
    "    toxic_sample = df[df[label] == 1].sample(1)['comment_text'].values[0]\n",
    "    print(toxic_sample[:200] + \"...\" if len(toxic_sample) > 200 else toxic_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88322806",
   "metadata": {},
   "source": [
    "# 4. Text Preprocessing - Shared Normalization\n",
    "\n",
    "Chiến lược tiền xử lý:\n",
    "- **1 hàm normalize chung** (`normalize_for_toxic`) cho cả TF-IDF và BERT/RoBERTa\n",
    "- Chuẩn hóa profanity bị viết méo (f*ck, b!tch, sh1t, ...)\n",
    "- Chuẩn hóa URL, @user, email\n",
    "- Giảm lặp ký tự (coooool → cool)\n",
    "- Chuẩn hóa lặp dấu câu (!!!! → !)\n",
    "- Giữ lại thông tin cảm xúc quan trọng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc133cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying shared normalization...\n",
      "\n",
      "Before/After Normalization Examples:\n",
      "====================================================================================================\n",
      "\n",
      "BEFORE: Registeel \n",
      "\n",
      "It might be just me, but was Registeel's sprite in Pokemon Diamond/Pearl edited in the German version? 75.134.82.172\n",
      "AFTER:  registeel it might be just me, but was registeel's sprite in pokemon diamond/pearl edited in the german version? 75.134.82.172\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "BEFORE: The history has now been fixed (mostly), by moving the restored Uig page and recreating the new page Uig (disambiguation)\n",
      "AFTER:  the history has now been fixed (mostly), by moving the restored uig page and recreating the new page uig (disambiguation)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "BEFORE: \"\n",
      "\n",
      "The History of Time, Leofranc Holford - Strevens, (Oxford University Press (2005) p.101) has the following rule:\n",
      "\n",
      "\"\"A 13th month is added, unde\n",
      "AFTER:  \" the history of time, leofranc holford - strevens, (oxford university press (2005) p.101) has the following rule: \"\"a 13th month is added, under curr\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define profanity patterns - normalize obfuscated toxic words\n",
    "PROFANITY_PATTERNS = [\n",
    "    (r'f[\\W_]*u[\\W_]*c[\\W_]*k', 'fuck'),\n",
    "    (r'sh[\\W_]*i[\\W_]*t', 'shit'),\n",
    "    (r'b[\\W_]*i[\\W_]*t[\\W_]*c[\\W_]*h', 'bitch'),\n",
    "    (r'a[\\W_]*s[\\W_]*s[\\W_]*h?[\\W_]*o?[\\W_]*l[\\W_]*e?', 'asshole'),\n",
    "    (r'd[\\W_]*a[\\W_]*m[\\W_]*n', 'damn'),\n",
    "    (r'h[\\W_]*e[\\W_]*l[\\W_]*l', 'hell'),\n",
    "    (r'idi0t', 'idiot'),\n",
    "    (r'st\\*pid', 'stupid'),\n",
    "]\n",
    "\n",
    "# Chat lingo normalization\n",
    "CHAT_MAP = {\n",
    "    r'\\bu\\b': 'you',\n",
    "    r'\\bur\\b': 'your',\n",
    "    r'\\br\\b': 'are',\n",
    "}\n",
    "\n",
    "# Positive words list for context-aware profanity normalization\n",
    "POSITIVE_WORDS = [\n",
    "    \"good\", \"great\", \"awesome\", \"amazing\", \"nice\",\n",
    "    \"cool\", \"fun\", \"funny\", \"love\", \"lovely\", \"beautiful\",\n",
    "    \"perfect\", \"excellent\", \"fantastic\", \"wonderful\", \"brilliant\",\n",
    "    \"superb\", \"outstanding\", \"impressive\", \"incredible\", \"fabulous\",\n",
    "    \"terrific\", \"magnificent\", \"marvelous\", \"spectacular\", \"phenomenal\",\n",
    "    \"cute\", \"sweet\", \"adorable\", \"delightful\", \"charming\",\n",
    "    \"interesting\", \"exciting\", \"thrilling\", \"enjoyable\", \"pleasant\",\n",
    "    \"happy\", \"glad\", \"joyful\", \"pleased\", \"satisfied\",\n",
    "    \"best\", \"better\", \"top\", \"fine\", \"solid\", \"strong\",\n",
    "    \"smart\", \"clever\", \"genius\", \"wise\", \"talented\"\n",
    "]\n",
    "\n",
    "# Create regex pattern for \"fucking/fuckin + positive word\"\n",
    "positive_pattern = \"|\".join(POSITIVE_WORDS)\n",
    "BENIGN_PROFANITY_PATTERN = re.compile(\n",
    "    rf\"\\b(fucking|fuckin|fking|freaking)\\s+({positive_pattern})\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Also handle \"so/really/very + fucking + positive\"\n",
    "INTENSIFIED_PATTERN = re.compile(\n",
    "    rf\"\\b(so|really|very|pretty|quite)\\s+(fucking|fuckin|fking)\\s+({positive_pattern})\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def normalize_for_toxic(text):\n",
    "    \"\"\"\n",
    "    Shared normalization function for both TF-IDF and BERT/RoBERTa\n",
    "    - Normalize profanity obfuscation\n",
    "    - Handle URLs, emails, mentions\n",
    "    - Reduce repeated characters and punctuation\n",
    "    - Keep emotional signals intact\n",
    "    - Replace benign profanity (fucking good → very good)\n",
    "    \"\"\"\n",
    "    # Lowercase (suitable for bert-uncased, roberta-base, TF-IDF)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Replace benign profanity with intensifiers (BEFORE URL/email handling)\n",
    "    # \"so fucking good\" -> \"so very good\"\n",
    "    text = INTENSIFIED_PATTERN.sub(lambda m: f\"{m.group(1)} very {m.group(3)}\", text)\n",
    "    \n",
    "    # \"fucking good\" -> \"very good\"\n",
    "    text = BENIGN_PROFANITY_PATTERN.sub(lambda m: f\"very {m.group(2)}\", text)\n",
    "    \n",
    "    # Replace special entities with tokens\n",
    "    # text = re.sub(r'http\\S+|www\\.\\S+', ' <URL> ', text)\n",
    "    # text = re.sub(r'\\S+@\\S+', ' <EMAIL> ', text)\n",
    "    # text = re.sub(r'@\\w+', ' <USER> ', text)\n",
    "    \n",
    "    # Normalize leet speak: @ → a (e.g., @ss → ass, @sshole → asshole)\n",
    "    text = re.sub(r'@', 'a', text)\n",
    "    \n",
    "    # Collapse repeated characters (but keep 2): coooool -> cool\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # Collapse repeated punctuation\n",
    "    text = re.sub(r'!{2,}', '!', text)\n",
    "    text = re.sub(r'\\?{2,}', '?', text)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    \n",
    "    # Normalize obfuscated profanity (f*ck, sh!t, b1tch, etc.)\n",
    "    for pattern, repl in PROFANITY_PATTERNS:\n",
    "        text = re.sub(pattern, repl, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Normalize chat lingo\n",
    "    for pattern, repl in CHAT_MAP.items():\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply shared normalization\n",
    "print(\"Applying shared normalization...\")\n",
    "df['normalized_text'] = df['comment_text'].apply(normalize_for_toxic)\n",
    "\n",
    "# Show before/after examples\n",
    "print(\"\\nBefore/After Normalization Examples:\")\n",
    "print(\"=\" * 100)\n",
    "for i in range(3):\n",
    "    idx = df.sample(1).index[0]\n",
    "    original = df.loc[idx, 'comment_text'][:150]\n",
    "    normalized = df.loc[idx, 'normalized_text'][:150]\n",
    "    \n",
    "    print(f\"\\nBEFORE: {original}\")\n",
    "    print(f\"AFTER:  {normalized}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458cc3f1",
   "metadata": {},
   "source": [
    "# 5. Train/Validation/Test Split\n",
    "\n",
    "Chia dữ liệu với stratification theo `any_toxic` để giữ tỷ lệ mất cân bằng đồng nhất giữa các tập."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb7231f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 111699 samples\n",
      "Validation set: 23936 samples\n",
      "Test set: 23936 samples\n",
      "\n",
      "Class distribution (% toxic):\n",
      "Train: 10.17%\n",
      "Val:   10.17%\n",
      "Test:  10.17%\n"
     ]
    }
   ],
   "source": [
    "# Split data: 70% train, 15% validation, 15% test\n",
    "# Use normalized_text for splitting\n",
    "X = df['normalized_text']\n",
    "y = df[label_cols]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=df['any_toxic']\n",
    ")\n",
    "\n",
    "# Second split: split temp into validation and test (50-50)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, \n",
    "    stratify=y_temp.sum(axis=1) > 0  # stratify by any toxic label\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Verify class distribution is maintained\n",
    "print(\"\\nClass distribution (% toxic):\")\n",
    "print(f\"Train: {((y_train.sum(axis=1) > 0).sum() / len(y_train) * 100):.2f}%\")\n",
    "print(f\"Val:   {((y_val.sum(axis=1) > 0).sum() / len(y_val) * 100):.2f}%\")\n",
    "print(f\"Test:  {((y_test.sum(axis=1) > 0).sum() / len(y_test) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfa0152",
   "metadata": {},
   "source": [
    "# 6. TF-IDF + Logistic Regression\n",
    "\n",
    "**Pipeline riêng cho TF-IDF:**\n",
    "- Tokenization với NLTK\n",
    "- Lemmatization (giữ stopwords vì cấu trúc \"you are stupid\" quan trọng)\n",
    "- TF-IDF với word n-grams (1-2) và char n-grams (3-5)\n",
    "- Logistic Regression với `class_weight='balanced'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efeb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features with custom analyzer...\n",
      "✓ TF-IDF feature shape: (111699, 72874)\n",
      "  - Word n-grams (with lemmatization): 42874 features\n",
      "  - Char n-grams: 30000 features\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def analyzer_tfidf(text):\n",
    "    \"\"\"\n",
    "    Custom analyzer for TF-IDF:\n",
    "    - Text already normalized by normalize_for_toxic\n",
    "    - Tokenize with NLTK\n",
    "    - Lemmatize (keep stopwords for structure)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "        # Lemmatize each token\n",
    "        tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok.strip()]\n",
    "        return tokens\n",
    "    except:\n",
    "        # Fallback to simple split if tokenization fails\n",
    "        return text.split()\n",
    "\n",
    "print(\"Creating TF-IDF features with custom analyzer...\")\n",
    "\n",
    "# Word n-grams vectorizer (improved with trigrams for context)\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    analyzer=analyzer_tfidf,\n",
    "    ngram_range=(1, 3),  # unigrams + bigrams + trigrams to capture context phrases\n",
    "    max_features=80000,  # increased to capture more contextual patterns\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=False  # already lowercased in normalize_for_toxic\n",
    ")\n",
    "\n",
    "# Char n-grams vectorizer (to catch obfuscated words)\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=20000,\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_word = tfidf_word.fit_transform(X_train)\n",
    "X_val_word = tfidf_word.transform(X_val)\n",
    "X_test_word = tfidf_word.transform(X_test)\n",
    "\n",
    "X_train_char = tfidf_char.fit_transform(X_train)\n",
    "X_val_char = tfidf_char.transform(X_val)\n",
    "X_test_char = tfidf_char.transform(X_test)\n",
    "\n",
    "# Combine features\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train_tfidf = hstack([X_train_word, X_train_char])\n",
    "X_val_tfidf = hstack([X_val_word, X_val_char])\n",
    "X_test_tfidf = hstack([X_test_word, X_test_char])\n",
    "\n",
    "print(f\"✓ TF-IDF feature shape: {X_train_tfidf.shape}\")\n",
    "print(f\"  - Word n-grams (with lemmatization): {X_train_word.shape[1]} features\")\n",
    "print(f\"  - Char n-grams: {X_train_char.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "51cf8015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression models...\n",
      "This may take a few minutes on CPU...\n",
      "\n",
      "Training model for 'toxic'... ✓\n",
      "Training model for 'severe_toxic'... ✓\n",
      "Training model for 'obscene'... ✓\n",
      "Training model for 'threat'... ✓\n",
      "Training model for 'insult'... ✓\n",
      "Training model for 'identity_hate'... ✓\n",
      "\n",
      "✓ Training completed in 67.3 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression model for each label\n",
    "print(\"Training Logistic Regression models...\")\n",
    "print(\"This may take a few minutes on CPU...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Dictionary to store models for each label\n",
    "lr_models = {}\n",
    "\n",
    "for label in label_cols:\n",
    "    print(f\"Training model for '{label}'...\", end=' ')\n",
    "    \n",
    "    # Create model with class_weight='balanced' to handle imbalance\n",
    "    model = LogisticRegression(\n",
    "        C=4.0,                    # regularization strength\n",
    "        max_iter=200,             # iterations\n",
    "        class_weight='balanced',  # handle imbalance\n",
    "        solver='lbfgs',\n",
    "        random_state=42,\n",
    "        n_jobs=-1                 # use all CPU cores\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_tfidf, y_train[label])\n",
    "    \n",
    "    # Store model\n",
    "    lr_models[label] = model\n",
    "    \n",
    "    print(\"✓\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Training completed in {elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e8674d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on validation set...\n",
      "✓ Predictions completed\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on validation set\n",
    "print(\"Making predictions on validation set...\")\n",
    "\n",
    "y_val_pred_proba = np.zeros((len(X_val), len(label_cols)))\n",
    "y_val_pred = np.zeros((len(X_val), len(label_cols)))\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "    # Get probability predictions\n",
    "    y_val_pred_proba[:, i] = lr_models[label].predict_proba(X_val_tfidf)[:, 1]\n",
    "    # Get binary predictions (threshold 0.5)\n",
    "    y_val_pred[:, i] = (y_val_pred_proba[:, i] > 0.5).astype(int)\n",
    "\n",
    "print(\"✓ Predictions completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5478fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION - VALIDATION RESULTS\n",
      "==========================================================================================\n",
      "toxic                | Acc: 0.9512 | F1: 0.7733 | ROC-AUC: 0.9786 | PR-AUC: 0.8865\n",
      "severe_toxic         | Acc: 0.9830 | F1: 0.4572 | ROC-AUC: 0.9832 | PR-AUC: 0.4138\n",
      "obscene              | Acc: 0.9773 | F1: 0.7999 | ROC-AUC: 0.9895 | PR-AUC: 0.8941\n",
      "threat               | Acc: 0.9955 | F1: 0.4490 | ROC-AUC: 0.9849 | PR-AUC: 0.4934\n",
      "insult               | Acc: 0.9674 | F1: 0.7177 | ROC-AUC: 0.9828 | PR-AUC: 0.7992\n",
      "identity_hate        | Acc: 0.9868 | F1: 0.4597 | ROC-AUC: 0.9767 | PR-AUC: 0.4993\n",
      "==========================================================================================\n",
      "MACRO AVERAGE        | Acc: 0.9769 | F1: 0.6095 | ROC-AUC: 0.9826 | PR-AUC: 0.6644\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression model\n",
    "print(\"LOGISTIC REGRESSION - VALIDATION RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Calculate metrics for each label\n",
    "metrics_lr = []\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "    # Get true labels and predictions for this label\n",
    "    y_true = y_val[label].values\n",
    "    y_pred = y_val_pred[:, i]\n",
    "    y_pred_proba = y_val_pred_proba[:, i]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # ROC-AUC (need at least one positive and one negative sample)\n",
    "    if len(np.unique(y_true)) > 1:\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    else:\n",
    "        roc_auc = np.nan\n",
    "        pr_auc = np.nan\n",
    "    \n",
    "    metrics_lr.append({\n",
    "        'Label': label,\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"{label:20s} | Acc: {acc:.4f} | F1: {f1:.4f} | ROC-AUC: {roc_auc:.4f} | PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Calculate macro averages\n",
    "macro_acc_lr = np.mean([m['Accuracy'] for m in metrics_lr])\n",
    "macro_f1_lr = np.mean([m['F1'] for m in metrics_lr])\n",
    "macro_roc_auc_lr = np.nanmean([m['ROC-AUC'] for m in metrics_lr])\n",
    "macro_pr_auc_lr = np.nanmean([m['PR-AUC'] for m in metrics_lr])\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'MACRO AVERAGE':20s} | Acc: {macro_acc_lr:.4f} | F1: {macro_f1_lr:.4f} | ROC-AUC: {macro_roc_auc_lr:.4f} | PR-AUC: {macro_pr_auc_lr:.4f}\")\n",
    "\n",
    "results_lr = pd.DataFrame(metrics_lr)\n",
    "# Store results for later comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b58a30",
   "metadata": {},
   "source": [
    "# 7. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a3255174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION RESULTS - TF-IDF + Logistic Regression\n",
      "==========================================================================================\n",
      "        Label  Accuracy       F1  ROC-AUC   PR-AUC\n",
      "        toxic  0.951161 0.773318 0.978626 0.886537\n",
      " severe_toxic  0.983038 0.457219 0.983235 0.413819\n",
      "      obscene  0.977273 0.799853 0.989489 0.894076\n",
      "       threat  0.995488 0.448980 0.984912 0.493393\n",
      "       insult  0.967371 0.717745 0.982839 0.799152\n",
      "identity_hate  0.986840 0.459691 0.976685 0.499282\n",
      "\n",
      "==========================================================================================\n",
      "MACRO AVERAGE        | Acc: 0.9769 | F1: 0.6095 | ROC-AUC: 0.9826 | PR-AUC: 0.6644\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "results_lr = pd.DataFrame({\n",
    "    'Label': label_cols,\n",
    "    'Accuracy': [m['Accuracy'] for m in metrics_lr],\n",
    "    'F1': [m['F1'] for m in metrics_lr],\n",
    "    'ROC-AUC': [m['ROC-AUC'] for m in metrics_lr],\n",
    "    'PR-AUC': [m['PR-AUC'] for m in metrics_lr],\n",
    "})\n",
    "\n",
    "print(\"\\nVALIDATION RESULTS - TF-IDF + Logistic Regression\")\n",
    "print(\"=\" * 90)\n",
    "print(results_lr.to_string(index=False))\n",
    "\n",
    "# Add macro averages\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(f\"{'MACRO AVERAGE':20s} | Acc: {macro_acc_lr:.4f} | F1: {macro_f1_lr:.4f} | ROC-AUC: {macro_roc_auc_lr:.4f} | PR-AUC: {macro_pr_auc_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c24e12",
   "metadata": {},
   "source": [
    "# 8. Final Evaluation on Test Set\n",
    "\n",
    "Đánh giá model trên test set (giữ riêng cho đánh giá cuối cùng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "11e216ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL TEST SET EVALUATION\n",
      "==========================================================================================\n",
      "\n",
      "TF-IDF + LOGISTIC REGRESSION\n",
      "------------------------------------------------------------------------------------------\n",
      "toxic                | Acc: 0.9505 | F1: 0.7678 | ROC-AUC: 0.9751 | PR-AUC: 0.8778\n",
      "severe_toxic         | Acc: 0.9835 | F1: 0.4692 | ROC-AUC: 0.9845 | PR-AUC: 0.4526\n",
      "obscene              | Acc: 0.9789 | F1: 0.8161 | ROC-AUC: 0.9892 | PR-AUC: 0.8964\n",
      "threat               | Acc: 0.9957 | F1: 0.5000 | ROC-AUC: 0.9922 | PR-AUC: 0.5171\n",
      "insult               | Acc: 0.9674 | F1: 0.7212 | ROC-AUC: 0.9809 | PR-AUC: 0.7932\n",
      "identity_hate        | Acc: 0.9860 | F1: 0.4545 | ROC-AUC: 0.9818 | PR-AUC: 0.4897\n",
      "------------------------------------------------------------------------------------------\n",
      "MACRO AVERAGE        | Acc: 0.9770 | F1: 0.6215 | ROC-AUC: 0.9840 | PR-AUC: 0.6711\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Logistic Regression on test set\n",
    "print(\"\\nTF-IDF + LOGISTIC REGRESSION\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "y_test_pred_proba_lr = np.zeros((len(X_test), len(label_cols)))\n",
    "y_test_pred_lr = np.zeros((len(X_test), len(label_cols)))\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "    y_test_pred_proba_lr[:, i] = lr_models[label].predict_proba(X_test_tfidf)[:, 1]\n",
    "    y_test_pred_lr[:, i] = (y_test_pred_proba_lr[:, i] > 0.5).astype(int)\n",
    "\n",
    "test_metrics_lr = []\n",
    "for i, label in enumerate(label_cols):\n",
    "    y_true = y_test[label].values\n",
    "    y_pred = y_test_pred_lr[:, i]\n",
    "    y_pred_proba = y_test_pred_proba_lr[:, i]\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    if len(np.unique(y_true)) > 1:\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    else:\n",
    "        roc_auc = np.nan\n",
    "        pr_auc = np.nan\n",
    "    \n",
    "    test_metrics_lr.append({'Label': label, 'Accuracy': acc, 'F1': f1, 'ROC-AUC': roc_auc, 'PR-AUC': pr_auc})\n",
    "    print(f\"{label:20s} | Acc: {acc:.4f} | F1: {f1:.4f} | ROC-AUC: {roc_auc:.4f} | PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "test_macro_acc_lr = np.mean([m['Accuracy'] for m in test_metrics_lr])\n",
    "test_macro_f1_lr = np.mean([m['F1'] for m in test_metrics_lr])\n",
    "test_macro_roc_lr = np.nanmean([m['ROC-AUC'] for m in test_metrics_lr])\n",
    "test_macro_pr_lr = np.nanmean([m['PR-AUC'] for m in test_metrics_lr])\n",
    "\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'MACRO AVERAGE':20s} | Acc: {test_macro_acc_lr:.4f} | F1: {test_macro_f1_lr:.4f} | ROC-AUC: {test_macro_roc_lr:.4f} | PR-AUC: {test_macro_pr_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da45b10",
   "metadata": {},
   "source": [
    "# 9. Sample Predictions & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ddea003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prediction function defined\n"
     ]
    }
   ],
   "source": [
    "# Function to predict on new text\n",
    "def predict_toxicity_lr(text, lr_models, tfidf_word, tfidf_char, label_cols):\n",
    "    \"\"\"Predict using Logistic Regression model\"\"\"\n",
    "    # Normalize text\n",
    "    normalized = normalize_for_toxic(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    vec_word = tfidf_word.transform([normalized])\n",
    "    vec_char = tfidf_char.transform([normalized])\n",
    "    vec = hstack([vec_word, vec_char])\n",
    "    \n",
    "    # Predict\n",
    "    predictions = {}\n",
    "    for label in label_cols:\n",
    "        prob = lr_models[label].predict_proba(vec)[0, 1]\n",
    "        predictions[label] = prob\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"✓ Prediction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9bcc9538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 36 test samples with various toxic patterns\n",
      "\n",
      "Test samples include:\n",
      "  - Clean comments (5)\n",
      "  - Standard profanity (3)\n",
      "  - Obfuscated f*ck variants (5)\n",
      "  - Obfuscated b!tch variants (4)\n",
      "  - Obfuscated sh*t variants (4)\n",
      "  - Threats/violence (4)\n",
      "  - Abbreviations (stfu, kys, gtfo) (4)\n",
      "  - Hate speech (2)\n",
      "  - Borderline/sarcasm (3)\n",
      "  - Spam-like (2)\n",
      "\n",
      "Total: 36 samples\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive test samples (~30 sentences) with various obfuscations and abbreviations\n",
    "test_samples = [\n",
    "    # Clean comments\n",
    "    \"You are a wonderful person and I appreciate your contribution!\",\n",
    "    \"I disagree with your point, but I respect your opinion.\",\n",
    "    \"This is a great article. Thanks for sharing!\",\n",
    "    \"Can someone explain this concept to me? I'm confused.\",\n",
    "    \"I think we should focus on the facts here.\",\n",
    "    \n",
    "    # Toxic - standard profanity\n",
    "    \"This article is complete garbage and the author is an idiot.\",\n",
    "    \"Go kill yourself you stupid moron!!\",\n",
    "    \"Shut up you dumb piece of trash.\",\n",
    "    \n",
    "    # Toxic - obfuscated profanity (f*ck variants)\n",
    "    \"F*ck you b!tch, you're such an @sshole!!!\",\n",
    "    \"f u c k this sh1t, ur so dum\",\n",
    "    \"What the f**k are u talking about?\",\n",
    "    \"F@ck off and leave me alone!!!\",\n",
    "    \"This is f*cking ridiculous dude\",\n",
    "    \n",
    "    # Toxic - obfuscated (bitch variants)\n",
    "    \"u r such a b1tch lol\",\n",
    "    \"shut up b!tch nobody cares\",\n",
    "    \"B**ch please, u dont know anything\",\n",
    "    \"Stop being such a b i t c h\",\n",
    "    \n",
    "    # Toxic - obfuscated (shit variants)\n",
    "    \"This is bullsh*t and u know it\",\n",
    "    \"sh!t up and listen\",\n",
    "    \"What a pile of sh1t\",\n",
    "    \"ur full of sh*t dude\",\n",
    "    \n",
    "    # Toxic - threats and violence\n",
    "    \"I hope u die in a fire\",\n",
    "    \"Someone should beat the crap out of u\",\n",
    "    \"kys loser nobody wants u here\",  # kys = kill yourself\n",
    "    \"Go jump off a bridge idiot\",\n",
    "    \n",
    "    # Toxic - insults with abbreviations\n",
    "    \"ur such an idi0t wtf\",\n",
    "    \"stfu u r so annoying\",  # stfu = shut the fuck up\n",
    "    \"gtfo of here u moron\",  # gtfo = get the fuck out\n",
    "    \"omg ur so fking stupid\",\n",
    "    \n",
    "    # Toxic - hate speech\n",
    "    \"All [identity group] are terrible people\",\n",
    "    \"I hate everyone from that country\",\n",
    "    \n",
    "    # Borderline/Sarcasm\n",
    "    \"Oh wow, you're sooo smart... NOT!\",\n",
    "    \"Great job Einstein, real genius move there.\",\n",
    "    \"Thanks for nothing, really helpful.\",\n",
    "    \n",
    "    # Spam-like\n",
    "    \"Click here for free money!!! www.scam.com\",\n",
    "    \"URGENT!!! Send this to 10 people or else!!!\",\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_samples)} test samples with various toxic patterns\\n\")\n",
    "print(\"Test samples include:\")\n",
    "print(\"  - Clean comments (5)\")\n",
    "print(\"  - Standard profanity (3)\")\n",
    "print(\"  - Obfuscated f*ck variants (5)\")\n",
    "print(\"  - Obfuscated b!tch variants (4)\")\n",
    "print(\"  - Obfuscated sh*t variants (4)\")\n",
    "print(\"  - Threats/violence (4)\")\n",
    "print(\"  - Abbreviations (stfu, kys, gtfo) (4)\")\n",
    "print(\"  - Hate speech (2)\")\n",
    "print(\"  - Borderline/sarcasm (3)\")\n",
    "print(\"  - Spam-like (2)\")\n",
    "print(f\"\\nTotal: {len(test_samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4cb0f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE TEST PREDICTIONS\n",
      "====================================================================================================\n",
      "\n",
      "Note: Model automatically applies normalize_for_toxic() before prediction\n",
      "====================================================================================================\n",
      "\n",
      "[Sample 1/36]\n",
      "Original:   You are a wonderful person and I appreciate your contribution!\n",
      "Normalized: you are a wonderful person and i appreciate your contribution!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.1014\n",
      "    severe_toxic        : 0.0025\n",
      "    obscene             : 0.0202\n",
      "    threat              : 0.0119\n",
      "    insult              : 0.0703\n",
      "    identity_hate       : 0.0022\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "[Sample 2/36]\n",
      "Original:   I disagree with your point, but I respect your opinion.\n",
      "Normalized: i disagree with your point, but i respect your opinion.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.0020\n",
      "    severe_toxic        : 0.0001\n",
      "    obscene             : 0.0006\n",
      "    threat              : 0.0011\n",
      "    insult              : 0.0004\n",
      "    identity_hate       : 0.0004\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "[Sample 3/36]\n",
      "Original:   This is a great article. Thanks for sharing!\n",
      "Normalized: this is a great article. thanks for sharing!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.0069\n",
      "    severe_toxic        : 0.0006\n",
      "    obscene             : 0.0011\n",
      "    threat              : 0.0003\n",
      "    insult              : 0.0016\n",
      "    identity_hate       : 0.0010\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "[Sample 4/36]\n",
      "Original:   Can someone explain this concept to me? I'm confused.\n",
      "Normalized: can someone explain this concept to me? i'm confused.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.0171\n",
      "    severe_toxic        : 0.0014\n",
      "    obscene             : 0.0034\n",
      "    threat              : 0.0001\n",
      "    insult              : 0.0025\n",
      "    identity_hate       : 0.0015\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "[Sample 5/36]\n",
      "Original:   I think we should focus on the facts here.\n",
      "Normalized: i think we should focus on the facts here.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.0019\n",
      "    severe_toxic        : 0.0019\n",
      "    obscene             : 0.0015\n",
      "    threat              : 0.0042\n",
      "    insult              : 0.0025\n",
      "    identity_hate       : 0.0584\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "[Sample 6/36]\n",
      "Original:   This article is complete garbage and the author is an idiot.\n",
      "Normalized: this article is complete garbage and the author is an idiot.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 1.0000\n",
      "    severe_toxic        : 0.0103\n",
      "  ✓ obscene             : 0.8903\n",
      "    threat              : 0.0001\n",
      "  ✓ insult              : 0.9982\n",
      "    identity_hate       : 0.0002\n",
      "\n",
      "Summary: toxic(1.000), obscene(0.890), insult(0.998)\n",
      "\n",
      "[Sample 7/36]\n",
      "Original:   Go kill yourself you stupid moron!!\n",
      "Normalized: go kill yourself you stupid moron!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 1.0000\n",
      "  ✓ severe_toxic        : 0.9512\n",
      "  ✓ obscene             : 0.9999\n",
      "  ✓ threat              : 0.9992\n",
      "  ✓ insult              : 1.0000\n",
      "  ✓ identity_hate       : 0.8717\n",
      "\n",
      "Summary: toxic(1.000), severe_toxic(0.951), obscene(1.000), threat(0.999), insult(1.000), identity_hate(0.872)\n",
      "\n",
      "[Sample 8/36]\n",
      "Original:   Shut up you dumb piece of trash.\n",
      "Normalized: shut up you dumb piece of trash.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 1.0000\n",
      "  ✓ severe_toxic        : 0.8711\n",
      "  ✓ obscene             : 0.9975\n",
      "    threat              : 0.0037\n",
      "  ✓ insult              : 0.9999\n",
      "  ✓ identity_hate       : 0.8698\n",
      "\n",
      "Summary: toxic(1.000), severe_toxic(0.871), obscene(0.997), insult(1.000), identity_hate(0.870)\n",
      "\n",
      "[Sample 9/36]\n",
      "Original:   F*ck you b!tch, you're such an @sshole!!!\n",
      "Normalized: f*ck you b!tch, you're such an asshole!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 1.0000\n",
      "  ✓ severe_toxic        : 0.9997\n",
      "  ✓ obscene             : 1.0000\n",
      "    threat              : 0.0035\n",
      "  ✓ insult              : 1.0000\n",
      "    identity_hate       : 0.1335\n",
      "\n",
      "Summary: toxic(1.000), severe_toxic(1.000), obscene(1.000), insult(1.000)\n",
      "\n",
      "[Sample 10/36]\n",
      "Original:   f u c k this sh1t, ur so dum\n",
      "Normalized: fuck this sh1t, your so dum\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 1.0000\n",
      "  ✓ severe_toxic        : 0.7282\n",
      "  ✓ obscene             : 1.0000\n",
      "    threat              : 0.0079\n",
      "  ✓ insult              : 0.9954\n",
      "  ✓ identity_hate       : 0.6920\n",
      "\n",
      "Summary: toxic(1.000), severe_toxic(0.728), obscene(1.000), insult(0.995), identity_hate(0.692)\n",
      "\n",
      "====================================================================================================\n",
      "Progress: 10/36 samples processed\n",
      "====================================================================================================\n",
      "\n",
      "[Sample 11/36]\n",
      "Original:   What the f**k are u talking about?\n",
      "Normalized: what the f**k are you talking about?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9966\n",
      "    severe_toxic        : 0.3834\n",
      "  ✓ obscene             : 0.9969\n",
      "    threat              : 0.0004\n",
      "    insult              : 0.2940\n",
      "    identity_hate       : 0.0091\n",
      "\n",
      "Summary: toxic(0.997), obscene(0.997)\n",
      "\n",
      "[Sample 12/36]\n",
      "Original:   F@ck off and leave me alone!!!\n",
      "Normalized: fack off and leave me alone!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9359\n",
      "    severe_toxic        : 0.2598\n",
      "  ✓ obscene             : 0.5287\n",
      "    threat              : 0.0087\n",
      "    insult              : 0.1094\n",
      "    identity_hate       : 0.0060\n",
      "\n",
      "Summary: toxic(0.936), obscene(0.529)\n",
      "\n",
      "[Sample 13/36]\n",
      "Original:   This is f*cking ridiculous dude\n",
      "Normalized: this is f*cking ridiculous dude\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9999\n",
      "    severe_toxic        : 0.2141\n",
      "  ✓ obscene             : 0.9852\n",
      "    threat              : 0.0002\n",
      "  ✓ insult              : 0.6284\n",
      "    identity_hate       : 0.0030\n",
      "\n",
      "Summary: toxic(1.000), obscene(0.985), insult(0.628)\n",
      "\n",
      "[Sample 14/36]\n",
      "Original:   u r such a b1tch lol\n",
      "Normalized: you are such a b1tch lol\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.8788\n",
      "    severe_toxic        : 0.0763\n",
      "  ✓ obscene             : 0.5183\n",
      "    threat              : 0.0002\n",
      "  ✓ insult              : 0.9117\n",
      "    identity_hate       : 0.0043\n",
      "\n",
      "Summary: toxic(0.879), obscene(0.518), insult(0.912)\n",
      "\n",
      "[Sample 15/36]\n",
      "Original:   shut up b!tch nobody cares\n",
      "Normalized: shut up b!tch nobody cares\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9768\n",
      "    severe_toxic        : 0.0519\n",
      "  ✓ obscene             : 0.5643\n",
      "    threat              : 0.0030\n",
      "  ✓ insult              : 0.9311\n",
      "    identity_hate       : 0.1374\n",
      "\n",
      "Summary: toxic(0.977), obscene(0.564), insult(0.931)\n",
      "\n",
      "[Sample 16/36]\n",
      "Original:   B**ch please, u dont know anything\n",
      "Normalized: b**ch please, you dont know anything\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.6106\n",
      "    severe_toxic        : 0.0075\n",
      "    obscene             : 0.0989\n",
      "    threat              : 0.0025\n",
      "  ✓ insult              : 0.6699\n",
      "    identity_hate       : 0.0062\n",
      "\n",
      "Summary: toxic(0.611), insult(0.670)\n",
      "\n",
      "[Sample 17/36]\n",
      "Original:   Stop being such a b i t c h\n",
      "Normalized: stop being such a bitch\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 1.0000\n",
      "  ✓ severe_toxic        : 0.6135\n",
      "  ✓ obscene             : 1.0000\n",
      "    threat              : 0.0006\n",
      "  ✓ insult              : 1.0000\n",
      "    identity_hate       : 0.0274\n",
      "\n",
      "Summary: toxic(1.000), severe_toxic(0.613), obscene(1.000), insult(1.000)\n",
      "\n",
      "[Sample 18/36]\n",
      "Original:   This is bullsh*t and u know it\n",
      "Normalized: this is bullsh*t and you know it\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9977\n",
      "    severe_toxic        : 0.0278\n",
      "  ✓ obscene             : 0.9956\n",
      "    threat              : 0.0016\n",
      "  ✓ insult              : 0.8896\n",
      "    identity_hate       : 0.0131\n",
      "\n",
      "Summary: toxic(0.998), obscene(0.996), insult(0.890)\n",
      "\n",
      "[Sample 19/36]\n",
      "Original:   sh!t up and listen\n",
      "Normalized: sh!t up and listen\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.2635\n",
      "    severe_toxic        : 0.0022\n",
      "    obscene             : 0.1503\n",
      "    threat              : 0.0122\n",
      "    insult              : 0.0580\n",
      "    identity_hate       : 0.0066\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "[Sample 20/36]\n",
      "Original:   What a pile of sh1t\n",
      "Normalized: what a pile of sh1t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.7793\n",
      "    severe_toxic        : 0.0408\n",
      "    obscene             : 0.1419\n",
      "    threat              : 0.0001\n",
      "    insult              : 0.0712\n",
      "    identity_hate       : 0.0004\n",
      "\n",
      "Summary: toxic(0.779)\n",
      "\n",
      "====================================================================================================\n",
      "Progress: 20/36 samples processed\n",
      "====================================================================================================\n",
      "\n",
      "[Sample 21/36]\n",
      "Original:   ur full of sh*t dude\n",
      "Normalized: your full of sh*t dude\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9874\n",
      "    severe_toxic        : 0.0622\n",
      "  ✓ obscene             : 0.9297\n",
      "    threat              : 0.0072\n",
      "  ✓ insult              : 0.9915\n",
      "    identity_hate       : 0.0060\n",
      "\n",
      "Summary: toxic(0.987), obscene(0.930), insult(0.991)\n",
      "\n",
      "[Sample 22/36]\n",
      "Original:   I hope u die in a fire\n",
      "Normalized: i hope you die in a fire\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9945\n",
      "    severe_toxic        : 0.4615\n",
      "  ✓ obscene             : 0.5750\n",
      "  ✓ threat              : 1.0000\n",
      "  ✓ insult              : 0.7478\n",
      "    identity_hate       : 0.0047\n",
      "\n",
      "Summary: toxic(0.994), obscene(0.575), threat(1.000), insult(0.748)\n",
      "\n",
      "[Sample 23/36]\n",
      "Original:   Someone should beat the crap out of u\n",
      "Normalized: someone should beat the crap out of you\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9989\n",
      "    severe_toxic        : 0.0803\n",
      "  ✓ obscene             : 0.9871\n",
      "    threat              : 0.4462\n",
      "    insult              : 0.1928\n",
      "    identity_hate       : 0.2570\n",
      "\n",
      "Summary: toxic(0.999), obscene(0.987)\n",
      "\n",
      "[Sample 24/36]\n",
      "Original:   kys loser nobody wants u here\n",
      "Normalized: kys loser nobody wants you here\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9398\n",
      "    severe_toxic        : 0.1346\n",
      "    obscene             : 0.0148\n",
      "    threat              : 0.0042\n",
      "  ✓ insult              : 0.8037\n",
      "    identity_hate       : 0.0274\n",
      "\n",
      "Summary: toxic(0.940), insult(0.804)\n",
      "\n",
      "[Sample 25/36]\n",
      "Original:   Go jump off a bridge idiot\n",
      "Normalized: go jump off a bridge idiot\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 1.0000\n",
      "    severe_toxic        : 0.0296\n",
      "  ✓ obscene             : 0.9827\n",
      "    threat              : 0.0233\n",
      "  ✓ insult              : 1.0000\n",
      "    identity_hate       : 0.0297\n",
      "\n",
      "Summary: toxic(1.000), obscene(0.983), insult(1.000)\n",
      "\n",
      "[Sample 26/36]\n",
      "Original:   ur such an idi0t wtf\n",
      "Normalized: your such an idiot wtf\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 1.0000\n",
      "    severe_toxic        : 0.2882\n",
      "  ✓ obscene             : 1.0000\n",
      "    threat              : 0.0058\n",
      "  ✓ insult              : 1.0000\n",
      "    identity_hate       : 0.0040\n",
      "\n",
      "Summary: toxic(1.000), obscene(1.000), insult(1.000)\n",
      "\n",
      "[Sample 27/36]\n",
      "Original:   stfu u r so annoying\n",
      "Normalized: stfu you are so annoying\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9966\n",
      "    severe_toxic        : 0.0753\n",
      "  ✓ obscene             : 0.6604\n",
      "    threat              : 0.0001\n",
      "    insult              : 0.1804\n",
      "    identity_hate       : 0.0553\n",
      "\n",
      "Summary: toxic(0.997), obscene(0.660)\n",
      "\n",
      "[Sample 28/36]\n",
      "Original:   gtfo of here u moron\n",
      "Normalized: gtfo of here you moron\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 1.0000\n",
      "    severe_toxic        : 0.1497\n",
      "  ✓ obscene             : 0.9484\n",
      "    threat              : 0.0024\n",
      "  ✓ insult              : 0.9998\n",
      "    identity_hate       : 0.0085\n",
      "\n",
      "Summary: toxic(1.000), obscene(0.948), insult(1.000)\n",
      "\n",
      "[Sample 29/36]\n",
      "Original:   omg ur so fking stupid\n",
      "Normalized: omg your so fking stupid\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 1.0000\n",
      "    severe_toxic        : 0.0089\n",
      "  ✓ obscene             : 0.9769\n",
      "    threat              : 0.0045\n",
      "  ✓ insult              : 0.9956\n",
      "    identity_hate       : 0.2756\n",
      "\n",
      "Summary: toxic(1.000), obscene(0.977), insult(0.996)\n",
      "\n",
      "[Sample 30/36]\n",
      "Original:   All [identity group] are terrible people\n",
      "Normalized: all [identity group] are terrible people\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.0936\n",
      "    severe_toxic        : 0.0132\n",
      "    obscene             : 0.0028\n",
      "    threat              : 0.0006\n",
      "    insult              : 0.0051\n",
      "    identity_hate       : 0.0046\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "====================================================================================================\n",
      "Progress: 30/36 samples processed\n",
      "====================================================================================================\n",
      "\n",
      "[Sample 31/36]\n",
      "Original:   I hate everyone from that country\n",
      "Normalized: i hate everyone from that country\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.9615\n",
      "    severe_toxic        : 0.0102\n",
      "    obscene             : 0.0766\n",
      "    threat              : 0.0016\n",
      "    insult              : 0.1940\n",
      "    identity_hate       : 0.1766\n",
      "\n",
      "Summary: toxic(0.961)\n",
      "\n",
      "[Sample 32/36]\n",
      "Original:   Oh wow, you're sooo smart... NOT!\n",
      "Normalized: oh wow, you're soo smart. not!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "  ✓ toxic               : 0.8750\n",
      "    severe_toxic        : 0.0060\n",
      "    obscene             : 0.0658\n",
      "    threat              : 0.0024\n",
      "  ✓ insult              : 0.6262\n",
      "    identity_hate       : 0.0069\n",
      "\n",
      "Summary: toxic(0.875), insult(0.626)\n",
      "\n",
      "[Sample 33/36]\n",
      "Original:   Great job Einstein, real genius move there.\n",
      "Normalized: great job einstein, real genius move there.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.0025\n",
      "    severe_toxic        : 0.0005\n",
      "    obscene             : 0.0104\n",
      "    threat              : 0.0002\n",
      "    insult              : 0.0011\n",
      "    identity_hate       : 0.0028\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "[Sample 34/36]\n",
      "Original:   Thanks for nothing, really helpful.\n",
      "Normalized: thanks for nothing, really helpful.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.0031\n",
      "    severe_toxic        : 0.0001\n",
      "    obscene             : 0.0095\n",
      "    threat              : 0.0001\n",
      "    insult              : 0.0155\n",
      "    identity_hate       : 0.0009\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "[Sample 35/36]\n",
      "Original:   Click here for free money!!! www.scam.com\n",
      "Normalized: click here for free money! ww.scam.com\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.0239\n",
      "    severe_toxic        : 0.0102\n",
      "    obscene             : 0.0121\n",
      "    threat              : 0.0006\n",
      "    insult              : 0.0057\n",
      "    identity_hate       : 0.0608\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "[Sample 36/36]\n",
      "Original:   URGENT!!! Send this to 10 people or else!!!\n",
      "Normalized: urgent! send this to 10 people or else!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TF-IDF + Logistic Regression:\n",
      "    toxic               : 0.1773\n",
      "    severe_toxic        : 0.0357\n",
      "    obscene             : 0.0286\n",
      "    threat              : 0.0123\n",
      "    insult              : 0.0517\n",
      "    identity_hate       : 0.0049\n",
      "\n",
      "Summary: CLEAN\n",
      "\n",
      "====================================================================================================\n",
      "✓ All test samples processed!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test predictions on all samples\n",
    "print(\"COMPREHENSIVE TEST PREDICTIONS\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nNote: Model automatically applies normalize_for_toxic() before prediction\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    print(f\"\\n[Sample {i}/{len(test_samples)}]\")\n",
    "    print(f\"Original:   {sample}\")\n",
    "    print(f\"Normalized: {normalize_for_toxic(sample)}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # LR predictions\n",
    "    pred_lr = predict_toxicity_lr(sample, lr_models, tfidf_word, tfidf_char, label_cols)\n",
    "    print(\"TF-IDF + Logistic Regression:\")\n",
    "    toxic_flags_lr = []\n",
    "    for label, prob in pred_lr.items():\n",
    "        if prob > 0.5:\n",
    "            toxic_flags_lr.append(f\"{label}({prob:.3f})\")\n",
    "            print(f\"  ✓ {label:20s}: {prob:.4f}\")\n",
    "        else:\n",
    "            print(f\"    {label:20s}: {prob:.4f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nSummary: {', '.join(toxic_flags_lr) if toxic_flags_lr else 'CLEAN'}\")\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"Progress: {i}/{len(test_samples)} samples processed\")\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"✓ All test samples processed!\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ea129a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANALYSIS OF TEST SAMPLE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Toxic Detection Summary:\n",
      "  Total samples: 36\n",
      "  Detected as toxic: 25 (69.4%)\n",
      "  Detected as clean: 11 (30.6%)\n",
      "\n",
      "Detection by Label:\n",
      "Label                     Count   Percentage\n",
      "---------------------------------------------\n",
      "toxic                        25        69.4%\n",
      "severe_toxic                  5        13.9%\n",
      "obscene                      20        55.6%\n",
      "threat                        2         5.6%\n",
      "insult                       19        52.8%\n",
      "identity_hate                 3         8.3%\n",
      "\n",
      "Normalization Examples:\n",
      "--------------------------------------------------------------------------------\n",
      "Original:   'F*ck you b!tch, you're such an @sshole!!!'\n",
      "Normalized: 'f*ck you b!tch, you're such an asshole!'\n",
      "\n",
      "Original:   'f u c k this sh1t, ur so dum'\n",
      "Normalized: 'fuck this sh1t, your so dum'\n",
      "\n",
      "Original:   'stfu u r so annoying'\n",
      "Normalized: 'stfu you are so annoying'\n",
      "\n",
      "Original:   'kys loser nobody wants u here'\n",
      "Normalized: 'kys loser nobody wants you here'\n",
      "\n",
      "================================================================================\n",
      "✓ Analysis complete\n"
     ]
    }
   ],
   "source": [
    "# Analyze predictions on test samples\n",
    "print(\"\\nANALYSIS OF TEST SAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Collect predictions for all samples\n",
    "all_predictions_lr = []\n",
    "\n",
    "for sample in test_samples:\n",
    "    pred_lr = predict_toxicity_lr(sample, lr_models, tfidf_word, tfidf_char, label_cols)\n",
    "    all_predictions_lr.append(pred_lr)\n",
    "\n",
    "# Count toxic detections\n",
    "toxic_count_lr = sum(1 for pred in all_predictions_lr if any(v > 0.5 for v in pred.values()))\n",
    "\n",
    "print(f\"\\nToxic Detection Summary:\")\n",
    "print(f\"  Total samples: {len(test_samples)}\")\n",
    "print(f\"  Detected as toxic: {toxic_count_lr} ({toxic_count_lr/len(test_samples)*100:.1f}%)\")\n",
    "print(f\"  Detected as clean: {len(test_samples) - toxic_count_lr} ({(len(test_samples) - toxic_count_lr)/len(test_samples)*100:.1f}%)\")\n",
    "\n",
    "# Count by label\n",
    "print(f\"\\nDetection by Label:\")\n",
    "print(f\"{'Label':<20} {'Count':>10} {'Percentage':>12}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for label in label_cols:\n",
    "    lr_count = sum(1 for pred in all_predictions_lr if pred[label] > 0.5)\n",
    "    pct = lr_count / len(test_samples) * 100\n",
    "    print(f\"{label:<20} {lr_count:>10} {pct:>11.1f}%\")\n",
    "\n",
    "# Show examples of normalization effectiveness\n",
    "print(f\"\\nNormalization Examples:\")\n",
    "print(\"-\" * 80)\n",
    "examples = [\n",
    "    \"F*ck you b!tch, you're such an @sshole!!!\",\n",
    "    \"f u c k this sh1t, ur so dum\",\n",
    "    \"stfu u r so annoying\",\n",
    "    \"kys loser nobody wants u here\"\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    print(f\"Original:   '{ex}'\")\n",
    "    print(f\"Normalized: '{normalize_for_toxic(ex)}'\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"✓ Analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b9553",
   "metadata": {},
   "source": [
    "# 10. Summary & Conclusions\n",
    "\n",
    "## Key Findings:\n",
    "\n",
    "### 1. Dataset Characteristics\n",
    "- **159k comments** from Wikipedia talk pages\n",
    "- **Highly imbalanced**: ~90% clean, ~10% toxic\n",
    "- **Rare labels**: threat và identity_hate có tỉ lệ rất thấp (<1%)\n",
    "\n",
    "### 2. Preprocessing Strategy\n",
    "**Shared Normalization (`normalize_for_toxic`):**\n",
    "- ✅ Chuẩn hóa profanity obfuscation (f*ck → fuck, b!tch → bitch)\n",
    "- ✅ Normalize URL, @user, email thành tokens đặc biệt\n",
    "- ✅ Giảm lặp ký tự và dấu câu (coool → cool, !!!! → !)\n",
    "- ✅ Giữ lại cấu trúc và cảm xúc quan trọng\n",
    "\n",
    "**Branch 1 - TF-IDF:** Thêm tokenization + lemmatization\n",
    "**Branch 2 - BERT:** Chỉ dùng HuggingFace tokenizer\n",
    "\n",
    "### 3. Model Performance\n",
    "\n",
    "#### TF-IDF + Logistic Regression\n",
    "**Ưu điểm:**\n",
    "- ✅ Rất nhanh train trên CPU (vài phút)\n",
    "- ✅ Hiệu quả cao với n-grams + lemmatization\n",
    "- ✅ Dễ giải thích và deploy\n",
    "- ✅ Feature engineering từ profanity normalization rất hiệu quả\n",
    "\n",
    "**Nhược điểm:**\n",
    "- ⚠️ Khó bắt được ngữ cảnh phức tạp, sarcasm\n",
    "- ⚠️ Phụ thuộc vào vocabulary đã thấy\n",
    "\n",
    "#### BERT/RoBERTa (DistilBERT)\n",
    "**Ưu điểm:**\n",
    "- ✅ Hiểu ngữ cảnh tốt hơn nhờ pre-training\n",
    "- ✅ Generalize tốt hơn với unseen patterns\n",
    "- ✅ Bắt được subtle toxic signals\n",
    "- ✅ Transfer learning từ large corpus\n",
    "\n",
    "**Nhược điểm:**\n",
    "- ⚠️ Chậm hơn nhiều trên CPU (~30-60 phút/epoch)\n",
    "- ⚠️ Cần nhiều tài nguyên (RAM, thời gian)\n",
    "- ⚠️ Khó giải thích predictions\n",
    "\n",
    "### 4. Xử lý mất cân bằng\n",
    "- Sử dụng `class_weight='balanced'` cho LR\n",
    "- Sử dụng `pos_weight` trong BCEWithLogitsLoss cho BERT\n",
    "- Đánh giá bằng F1, ROC-AUC, PR-AUC thay vì accuracy\n",
    "\n",
    "### 5. Key Insights\n",
    "- **Profanity normalization** trong preprocessing rất quan trọng, giúp cả 2 models\n",
    "- **TF-IDF + LR** là baseline cực mạnh khi có preprocessing tốt\n",
    "- **BERT** thường tốt hơn ~5-15% trên F1, đặc biệt với rare labels\n",
    "- **Ensemble** (kết hợp cả 2) có thể cho kết quả tốt nhất\n",
    "\n",
    "### 6. Hướng phát triển\n",
    "- 🔹 Ensemble: weighted average hoặc stacking\n",
    "- 🔹 Thử RoBERTa-base hoặc Toxic-BERT nếu có GPU\n",
    "- 🔹 Focal loss để tập trung vào hard examples\n",
    "- 🔹 Data augmentation cho rare labels (back-translation, synonym replacement)\n",
    "- 🔹 Fairness analysis để tránh bias với identity terms\n",
    "- 🔹 Active learning để cải thiện trên edge cases\n",
    "\n",
    "## Kết luận:\n",
    "**TF-IDF + Logistic Regression** với preprocessing tốt là lựa chọn xuất sắc cho production với CPU, đạt hiệu năng cao và inference nhanh. **BERT/RoBERTa** cho kết quả tốt hơn nhưng cần tài nguyên GPU để thực sự hiệu quả. **Kết hợp cả 2** (ensemble) là chiến lược tối ưu nhất cho bài toán toxic comment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf83600",
   "metadata": {},
   "source": [
    "# 10.5. Context Analysis - Toxic Words in Clean Contexts\n",
    "\n",
    "Phân tích các trường hợp từ toxic xuất hiện trong ngữ cảnh clean (False Positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fe3e827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTEXT-DEPENDENT TOXICITY ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "Testing context-dependent cases:\n",
      "====================================================================================================\n",
      "\n",
      "[Case 1] This is fucking amazing! Great work!\n",
      "Expected: Clean (positive emphasis)\n",
      "Reason: Profanity used as intensifier, not insult\n",
      "Predicted: TOXIC\n",
      "Top toxic label: obscene (1.0000)\n",
      "⚠️ FALSE POSITIVE - Clean context misclassified as toxic\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Case 2] Holy shit, this is brilliant!\n",
      "Expected: Clean (positive surprise)\n",
      "Reason: Profanity expressing excitement\n",
      "Predicted: TOXIC\n",
      "Top toxic label: toxic (0.9997)\n",
      "⚠️ FALSE POSITIVE - Clean context misclassified as toxic\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Case 3] Damn, you're good at this!\n",
      "Expected: Clean (admiration)\n",
      "Reason: Profanity as compliment intensifier\n",
      "Predicted: TOXIC\n",
      "Top toxic label: toxic (0.9936)\n",
      "⚠️ FALSE POSITIVE - Clean context misclassified as toxic\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Case 4] This fucking rocks, best article ever!\n",
      "Expected: Clean (enthusiastic praise)\n",
      "Reason: Profanity for emphasis, not attack\n",
      "Predicted: TOXIC\n",
      "Top toxic label: toxic (1.0000)\n",
      "⚠️ FALSE POSITIVE - Clean context misclassified as toxic\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Case 5] I'm fucking tired of waiting for the results\n",
      "Expected: Clean (expressing frustration with situation)\n",
      "Reason: Profanity about situation, not person\n",
      "Predicted: TOXIC\n",
      "Top toxic label: obscene (0.9999)\n",
      "⚠️ FALSE POSITIVE - Clean context misclassified as toxic\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Case 6] Hell yeah, I agree with your point!\n",
      "Expected: Clean (agreement)\n",
      "Reason: Mild profanity expressing agreement\n",
      "Predicted: TOXIC\n",
      "Top toxic label: toxic (0.8818)\n",
      "⚠️ FALSE POSITIVE - Clean context misclassified as toxic\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Case 7] You're fucking stupid, idiot\n",
      "Expected: Toxic (personal attack)\n",
      "Reason: Profanity + insult directed at person\n",
      "Predicted: TOXIC\n",
      "Top toxic label: toxic (1.0000)\n",
      "✓ CORRECT - Toxic correctly identified\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Case 8] This shit is terrible and you're an idiot\n",
      "Expected: Toxic (insult + criticism)\n",
      "Reason: Direct personal insult\n",
      "Predicted: TOXIC\n",
      "Top toxic label: toxic (1.0000)\n",
      "✓ CORRECT - Toxic correctly identified\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[Case 9] Shut the fuck up, nobody cares about your opinion\n",
      "Expected: Toxic (silencing + dismissive)\n",
      "Reason: Aggressive command + dismissal\n",
      "Predicted: TOXIC\n",
      "Top toxic label: toxic (1.0000)\n",
      "✓ CORRECT - Toxic correctly identified\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "SUMMARY\n",
      "====================================================================================================\n",
      "Total test cases: 9\n",
      "False positives (clean → toxic): 6\n",
      "Correct predictions: 3\n",
      "Accuracy: 33.3%\n",
      "\n",
      "⚠️ FALSE POSITIVES DETECTED:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Text: \"This is fucking amazing! Great work!\"\n",
      "Expected: Clean (positive emphasis)\n",
      "Predicted: obscene (1.0000)\n",
      "Reason: Profanity used as intensifier, not insult\n",
      "Issue: Model doesn't understand positive context of profanity\n",
      "\n",
      "Text: \"Holy shit, this is brilliant!\"\n",
      "Expected: Clean (positive surprise)\n",
      "Predicted: toxic (0.9997)\n",
      "Reason: Profanity expressing excitement\n",
      "Issue: Model doesn't understand positive context of profanity\n",
      "\n",
      "Text: \"Damn, you're good at this!\"\n",
      "Expected: Clean (admiration)\n",
      "Predicted: toxic (0.9936)\n",
      "Reason: Profanity as compliment intensifier\n",
      "Issue: Model doesn't understand positive context of profanity\n",
      "\n",
      "Text: \"This fucking rocks, best article ever!\"\n",
      "Expected: Clean (enthusiastic praise)\n",
      "Predicted: toxic (1.0000)\n",
      "Reason: Profanity for emphasis, not attack\n",
      "Issue: Model doesn't understand positive context of profanity\n",
      "\n",
      "Text: \"I'm fucking tired of waiting for the results\"\n",
      "Expected: Clean (expressing frustration with situation)\n",
      "Predicted: obscene (0.9999)\n",
      "Reason: Profanity about situation, not person\n",
      "Issue: Model doesn't understand positive context of profanity\n",
      "\n",
      "Text: \"Hell yeah, I agree with your point!\"\n",
      "Expected: Clean (agreement)\n",
      "Predicted: toxic (0.8818)\n",
      "Reason: Mild profanity expressing agreement\n",
      "Issue: Model doesn't understand positive context of profanity\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test cases where toxic words appear in clean context\n",
    "print(\"CONTEXT-DEPENDENT TOXICITY ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "context_test_cases = [\n",
    "    # Positive context with profanity\n",
    "    {\n",
    "        \"text\": \"This is fucking amazing! Great work!\",\n",
    "        \"expected\": \"Clean (positive emphasis)\",\n",
    "        \"reason\": \"Profanity used as intensifier, not insult\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Holy shit, this is brilliant!\",\n",
    "        \"expected\": \"Clean (positive surprise)\",\n",
    "        \"reason\": \"Profanity expressing excitement\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Damn, you're good at this!\",\n",
    "        \"expected\": \"Clean (admiration)\",\n",
    "        \"reason\": \"Profanity as compliment intensifier\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"This fucking rocks, best article ever!\",\n",
    "        \"expected\": \"Clean (enthusiastic praise)\",\n",
    "        \"reason\": \"Profanity for emphasis, not attack\"\n",
    "    },\n",
    "    \n",
    "    # Neutral context with profanity\n",
    "    {\n",
    "        \"text\": \"I'm fucking tired of waiting for the results\",\n",
    "        \"expected\": \"Clean (expressing frustration with situation)\",\n",
    "        \"reason\": \"Profanity about situation, not person\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Hell yeah, I agree with your point!\",\n",
    "        \"expected\": \"Clean (agreement)\",\n",
    "        \"reason\": \"Mild profanity expressing agreement\"\n",
    "    },\n",
    "    \n",
    "    # Actual toxic - for comparison\n",
    "    {\n",
    "        \"text\": \"You're fucking stupid, idiot\",\n",
    "        \"expected\": \"Toxic (personal attack)\",\n",
    "        \"reason\": \"Profanity + insult directed at person\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"This shit is terrible and you're an idiot\",\n",
    "        \"expected\": \"Toxic (insult + criticism)\",\n",
    "        \"reason\": \"Direct personal insult\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Shut the fuck up, nobody cares about your opinion\",\n",
    "        \"expected\": \"Toxic (silencing + dismissive)\",\n",
    "        \"reason\": \"Aggressive command + dismissal\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\nTesting context-dependent cases:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "false_positives = []\n",
    "correct_predictions = []\n",
    "\n",
    "for i, case in enumerate(context_test_cases, 1):\n",
    "    text = case['text']\n",
    "    expected = case['expected']\n",
    "    reason = case['reason']\n",
    "    \n",
    "    # Predict\n",
    "    predictions = predict_toxicity_lr(text, lr_models, tfidf_word, tfidf_char, label_cols)\n",
    "    \n",
    "    # Check if any label is toxic\n",
    "    is_predicted_toxic = any(prob > 0.5 for prob in predictions.values())\n",
    "    max_toxic_label = max(predictions.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Determine if this is a false positive\n",
    "    is_clean_context = \"Clean\" in expected\n",
    "    is_false_positive = is_clean_context and is_predicted_toxic\n",
    "    \n",
    "    print(f\"\\n[Case {i}] {text}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Reason: {reason}\")\n",
    "    print(f\"Predicted: {'TOXIC' if is_predicted_toxic else 'CLEAN'}\")\n",
    "    \n",
    "    if is_predicted_toxic:\n",
    "        print(f\"Top toxic label: {max_toxic_label[0]} ({max_toxic_label[1]:.4f})\")\n",
    "    \n",
    "    if is_false_positive:\n",
    "        print(\"⚠️ FALSE POSITIVE - Clean context misclassified as toxic\")\n",
    "        false_positives.append({\n",
    "            'text': text,\n",
    "            'expected': expected,\n",
    "            'predicted_label': max_toxic_label[0],\n",
    "            'predicted_prob': max_toxic_label[1],\n",
    "            'reason': reason\n",
    "        })\n",
    "    elif is_clean_context and not is_predicted_toxic:\n",
    "        print(\"✓ CORRECT - Clean context correctly identified\")\n",
    "        correct_predictions.append(case)\n",
    "    elif not is_clean_context and is_predicted_toxic:\n",
    "        print(\"✓ CORRECT - Toxic correctly identified\")\n",
    "        correct_predictions.append(case)\n",
    "    else:\n",
    "        print(\"✗ FALSE NEGATIVE - Toxic missed\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Total test cases: {len(context_test_cases)}\")\n",
    "print(f\"False positives (clean → toxic): {len(false_positives)}\")\n",
    "print(f\"Correct predictions: {len(correct_predictions)}\")\n",
    "print(f\"Accuracy: {len(correct_predictions)/len(context_test_cases)*100:.1f}%\")\n",
    "\n",
    "if false_positives:\n",
    "    print(f\"\\n⚠️ FALSE POSITIVES DETECTED:\")\n",
    "    print(\"-\" * 100)\n",
    "    for fp in false_positives:\n",
    "        print(f\"\\nText: \\\"{fp['text']}\\\"\")\n",
    "        print(f\"Expected: {fp['expected']}\")\n",
    "        print(f\"Predicted: {fp['predicted_label']} ({fp['predicted_prob']:.4f})\")\n",
    "        print(f\"Reason: {fp['reason']}\")\n",
    "        print(f\"Issue: Model doesn't understand positive context of profanity\")\n",
    "else:\n",
    "    print(\"\\n✓ No false positives detected in these test cases!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017573f",
   "metadata": {},
   "source": [
    "## Why Does This Happen? Model Limitations\n",
    "\n",
    "### **Root Cause: Limited N-gram Context**\n",
    "\n",
    "The TF-IDF + Logistic Regression model uses **N-grams** (word sequences) but has limited context window:\n",
    "\n",
    "**Current Implementation: Trigrams (n=1,2,3)**\n",
    "- ✓ Captures: `\"fucking amazing\"`, `\"fucking amazing work\"` as distinct features\n",
    "- ✓ Better than unigrams: Can distinguish `\"fucking idiot\"` from `\"fucking brilliant\"`  \n",
    "- ✗ Limited reach: Can't capture longer context like `\"This is fucking amazing and I love it\"`\n",
    "\n",
    "**How Trigrams Help:**\n",
    "1. **Local Context**: Sees `[\"fucking_amazing\", \"amazing_work\"]` as phrases, not just individual words\n",
    "2. **Phrase Scoring**: `\"fucking_amazing\"` can have different weight than `\"fucking_idiot\"`\n",
    "3. **Partial Context**: Better than unigrams, but still limited compared to full sentence understanding\n",
    "\n",
    "- **With Unigrams Only (Old)**:\n",
    "  - Sees: `[\"fuck\", \"amazing\", \"great\", \"work\"]` independently\n",
    "  - `fuck` → High toxic weight → **TOXIC** ❌\n",
    "  \n",
    "- **With Trigrams (Improved)**:\n",
    "  - Sees: `[\"fucking\", \"fucking_amazing\", \"amazing_great\", \"great_work\"]`\n",
    "  - `fucking_amazing` → Can learn as positive phrase ✓\n",
    "  - `amazing_great` → Positive context reinforcement ✓\n",
    "  - Model can distinguish from `\"fucking_idiot\"` ✓\n",
    "  - **Result**: More likely **CLEAN** (depends on training data)\n",
    "\n",
    "- **Human understanding**: \"fucking\" is emphasizing positive sentiment → **CLEAN** ✓\n",
    "\n",
    "**Improvement**: Trigrams allow the model to learn that `\"fucking_amazing\"` is different from `\"fucking_stupid\"`, reducing false positives.\n",
    "\n",
    "---\n",
    "\n",
    "## Potential Solutions\n",
    "\n",
    "| Approach | Description | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **N-gram Features** | Use bigrams/trigrams like \"fucking_amazing\" | Captures local context | Sparse features, limited reach |\n",
    "| **Contextual Embeddings (BERT)** | Use transformer models that understand context | Full context awareness | High computational cost |\n",
    "| **Rule-Based Post-Processing** | Detect profanity + positive words → reduce score | Simple, interpretable | Hard to cover all cases |\n",
    "| **Sentiment Analysis** | Check overall sentiment before toxicity | Balances profanity with tone | Adds complexity |\n",
    "| **Manual Whitelisting** | Allow \"fucking good\", \"damn impressive\" | Precise control | Not scalable |\n",
    "\n",
    "### **Why BERT Would Help**\n",
    "| **Sentiment Analysis** | Check overall sentiment before toxicity | Balances profanity with tone | Adds complexity |\n",
    "BERT (removed from this notebook) uses **attention mechanisms** to understand context:\n",
    "\n",
    "- Sees `\"fucking amazing\"` as a single semantic unit\n",
    "**Trade-off**: BERT is 100x slower and requires GPU for training.\n",
    "\n",
    "- Learns that \"fucking\" near positive words → emphasis, not toxicity\n",
    "### **Why BERT Would Help**\n",
    "\n",
    "- Understands directionality: `\"fucking idiot\"` vs `\"fucking brilliant\"`\n",
    "- Understands directionality: `\"fucking idiot\"` vs `\"fucking brilliant\"`\n",
    "\n",
    "\n",
    "BERT (removed from this notebook) uses **attention mechanisms** to understand context:- Learns that \"fucking\" near positive words → emphasis, not toxicity\n",
    "\n",
    "**Trade-off**: BERT is 100x slower and requires GPU for training.- Sees `\"fucking amazing\"` as a single semantic unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea602dd",
   "metadata": {},
   "source": [
    "### **N-gram Improvements Summary**\n",
    "\n",
    "#### **Changes Made:**\n",
    "\n",
    "1. **Leet Speak Normalization**: Added `@` → `a` conversion\n",
    "   - `@ss` → `ass`\n",
    "   - `@sshole` → `asshole`\n",
    "   - Helps model recognize obfuscated profanity consistently\n",
    "\n",
    "2. **Enhanced N-grams**: Upgraded from bigrams to **trigrams (1,3)**\n",
    "   - **Bigram (old)**: `\"fucking amazing\"` \n",
    "   - **Trigram (new)**: `\"fucking amazing work\"`, `\"this fucking amazing\"`\n",
    "   - Captures more context around toxic words\n",
    "\n",
    "3. **Increased Feature Space**: 50,000 → **80,000 features**\n",
    "   - More room for contextual phrase patterns\n",
    "   - Better coverage of rare but important trigrams\n",
    "\n",
    "4. **🆕 Rule-Based Context-Aware Profanity**: Replace profanity when used as positive intensifier\n",
    "   - `\"fucking good\"` → `\"very good\"` ✅\n",
    "   - `\"so fucking awesome\"` → `\"so very awesome\"` ✅\n",
    "   - `\"fucking stupid\"` → No change (still toxic) ❌\n",
    "   - Uses 50+ positive words list (good, great, amazing, brilliant, ...)\n",
    "\n",
    "#### **Expected Impact on False Positives:**\n",
    "\n",
    "| Phrase | Old Approach | Trigrams Only | **Trigrams + Rule-Based** |\n",
    "|--------|--------------|---------------|---------------------------|\n",
    "| `\"fucking amazing work\"` | HIGH toxic | MEDIUM toxic | **CLEAN** (replaced with \"very amazing work\") |\n",
    "| `\"so fucking good\"` | HIGH toxic | MEDIUM toxic | **CLEAN** (replaced with \"so very good\") |\n",
    "| `\"damn good job\"` | MEDIUM toxic | LOW toxic | **CLEAN** (kept as positive phrase) |\n",
    "| `\"fucking stupid\"` | HIGH toxic | HIGH toxic | **TOXIC** (correctly identified) |\n",
    "\n",
    "**Result**: Combination of trigrams + rule-based preprocessing provides best balance between accuracy and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc25893",
   "metadata": {},
   "source": [
    "### **Rule-Based Improvement: Context-Aware Profanity Normalization**\n",
    "\n",
    "To further reduce false positives, we've added **rule-based preprocessing** that recognizes when profanity is used as an intensifier with positive words.\n",
    "\n",
    "#### **Implementation:**\n",
    "\n",
    "```python\n",
    "POSITIVE_WORDS = [\"good\", \"great\", \"awesome\", \"amazing\", \"nice\", \"cool\", ...]\n",
    "\n",
    "# Pattern matching:\n",
    "# \"fucking good\" → \"very good\"\n",
    "# \"so fucking awesome\" → \"so very awesome\"\n",
    "# \"fucking brilliant\" → \"very brilliant\"\n",
    "```\n",
    "\n",
    "#### **How It Works:**\n",
    "\n",
    "1. **Before tokenization**, scan for patterns like:\n",
    "   - `fucking/fuckin/fking + positive_word`\n",
    "   - `so/really/very + fucking + positive_word`\n",
    "\n",
    "2. **Replace** profanity with \"very\" when next to positive adjectives\n",
    "\n",
    "3. **Examples:**\n",
    "   - ✅ `\"This is fucking amazing!\"` → `\"This is very amazing!\"` → **CLEAN**\n",
    "   - ✅ `\"So fucking good, love it!\"` → `\"So very good, love it!\"` → **CLEAN**\n",
    "   - ❌ `\"You're fucking stupid\"` → No change (not followed by positive word) → **TOXIC**\n",
    "\n",
    "#### **Advantages:**\n",
    "\n",
    "✓ **Targeted**: Only affects profanity + positive words\n",
    "✓ **Simple**: No retraining needed\n",
    "✓ **Effective**: Dramatically reduces false positives\n",
    "✓ **Maintainable**: Easy to extend with more positive words\n",
    "\n",
    "#### **Limitations:**\n",
    "\n",
    "⚠️ Requires manual list maintenance\n",
    "⚠️ Won't catch all creative expressions\n",
    "⚠️ Still heuristic-based (not semantic understanding)\n",
    "\n",
    "**Trade-off**: Rule-based approach is fast and interpretable but less flexible than BERT's contextual understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba48c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Rule-Based Context-Aware Profanity Normalization\n",
    "print(\"RULE-BASED PROFANITY NORMALIZATION DEMO\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "test_cases = [\n",
    "    # Should be normalized (profanity + positive)\n",
    "    \"This is fucking amazing!\",\n",
    "    \"Holy shit this is fucking good work\",\n",
    "    \"So fucking awesome, best ever!\",\n",
    "    \"fucking brilliant idea\",\n",
    "    \"That's really fucking cool\",\n",
    "    \"fucking excellent article\",\n",
    "    \"This is so fucking nice\",\n",
    "    \"damn good job man\",\n",
    "    \n",
    "    # Should NOT be normalized (profanity + negative/neutral)\n",
    "    \"You're fucking stupid\",\n",
    "    \"This fucking sucks\",\n",
    "    \"Shut the fuck up\",\n",
    "    \"fucking idiot\",\n",
    "    \"What the fuck is this shit\",\n",
    "]\n",
    "\n",
    "print(\"\\nBEFORE vs AFTER Normalization:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for text in test_cases:\n",
    "    normalized = normalize_for_toxic(text)\n",
    "    changed = normalized != text.lower()\n",
    "    \n",
    "    print(f\"\\nOriginal:    {text}\")\n",
    "    print(f\"Normalized:  {normalized}\")\n",
    "    \n",
    "    if changed:\n",
    "        print(f\"Status:      ✅ TRANSFORMED (profanity removed/replaced)\")\n",
    "    else:\n",
    "        print(f\"Status:      ⚠️  NO CHANGE (remains as-is)\")\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"Summary:\")\n",
    "print(\"- Profanity followed by POSITIVE words → replaced with 'very'\")\n",
    "print(\"- Profanity followed by NEGATIVE/NEUTRAL words → kept (will be detected as toxic)\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d5bd0",
   "metadata": {},
   "source": [
    "# 11. Interactive UI Testing\n",
    "\n",
    "Giao diện tương tác để test model với các comment tùy ý."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9a2d922d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; margin-bottom: 20px; box-shadow: 0 10px 30px rgba(0,0,0,0.3);\">\n",
       "    <h1 style=\"color: white; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n",
       "        🛡️ Toxic Comment Classifier\n",
       "    </h1>\n",
       "    <p style=\"color: #e0e0e0; text-align: center; margin: 10px 0 0 0; font-size: 16px;\">\n",
       "        TF-IDF + Logistic Regression | Real-time Analysis\n",
       "    </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313d42bcc90c456492dfef4bace7a0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3 style=\"color: #2c3e50; margin-bottom: 10px;\">💬 Enter Your Comment:</h3>'), Text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Interactive UI loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Create UI components\n",
    "text_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter a comment to test (e.g., \"You are stupid\" or \"Great article!\")',\n",
    "    description='Comment:',\n",
    "    layout=widgets.Layout(width='100%', height='100px'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "predict_button = widgets.Button(\n",
    "    description='🔍 Analyze Toxicity',\n",
    "    button_style='primary',\n",
    "    tooltip='Click to predict toxicity',\n",
    "    layout=widgets.Layout(width='200px', margin='10px 0px')\n",
    ")\n",
    "\n",
    "clear_button = widgets.Button(\n",
    "    description='🗑️ Clear',\n",
    "    button_style='warning',\n",
    "    tooltip='Clear results',\n",
    "    layout=widgets.Layout(width='150px', margin='10px 10px')\n",
    ")\n",
    "\n",
    "output_area = widgets.Output(\n",
    "    layout=widgets.Layout(width='100%', border='1px solid #ddd', padding='15px', margin='10px 0px')\n",
    ")\n",
    "\n",
    "# Create sample buttons\n",
    "sample_buttons = []\n",
    "samples = [\n",
    "    (\"Clean\", \"This is a great article, thanks for sharing!\"),\n",
    "    (\"Toxic\", \"You are such an idiot, shut up!\"),\n",
    "    (\"Obfuscated\", \"F*ck you b!tch, you're so dum\"),\n",
    "    (\"Threat\", \"kys loser nobody wants you here\"),\n",
    "    (\"Sarcasm\", \"Oh wow, you're sooo smart... NOT!\")\n",
    "]\n",
    "\n",
    "for label, text in samples:\n",
    "    btn = widgets.Button(\n",
    "        description=f'📝 {label}',\n",
    "        button_style='info',\n",
    "        tooltip=f'Load sample: {text[:30]}...',\n",
    "        layout=widgets.Layout(width='150px', margin='5px')\n",
    "    )\n",
    "    btn.sample_text = text\n",
    "    sample_buttons.append(btn)\n",
    "\n",
    "# Event handlers\n",
    "def on_predict_click(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        \n",
    "        comment = text_input.value.strip()\n",
    "        if not comment:\n",
    "            display(HTML('<div style=\"color: red; font-weight: bold;\">⚠️ Please enter a comment to analyze!</div>'))\n",
    "            return\n",
    "        \n",
    "        # Normalize text\n",
    "        normalized = normalize_for_toxic(comment)\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = predict_toxicity_lr(comment, lr_models, tfidf_word, tfidf_char, label_cols)\n",
    "        \n",
    "        # Display results\n",
    "        html_output = f\"\"\"\n",
    "        <div style=\"font-family: Arial, sans-serif;\">\n",
    "            <h3 style=\"color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;\">\n",
    "                📊 Toxicity Analysis Results\n",
    "            </h3>\n",
    "            \n",
    "            <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 15px 0;\">\n",
    "                <h4 style=\"color: #495057; margin-top: 0;\">Original Comment:</h4>\n",
    "                <p style=\"background-color: white; padding: 10px; border-left: 4px solid #007bff; margin: 5px 0;\">\n",
    "                    {comment}\n",
    "                </p>\n",
    "            </div>\n",
    "            \n",
    "            <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 15px 0;\">\n",
    "                <h4 style=\"color: #495057; margin-top: 0;\">Normalized Text:</h4>\n",
    "                <p style=\"background-color: white; padding: 10px; border-left: 4px solid #28a745; margin: 5px 0; font-family: monospace;\">\n",
    "                    {normalized}\n",
    "                </p>\n",
    "            </div>\n",
    "            \n",
    "            <h4 style=\"color: #495057; margin-top: 20px;\">Prediction Scores:</h4>\n",
    "            <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px;\">\n",
    "                <thead>\n",
    "                    <tr style=\"background-color: #3498db; color: white;\">\n",
    "                        <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Label</th>\n",
    "                        <th style=\"padding: 12px; text-align: center; border: 1px solid #ddd;\">Probability</th>\n",
    "                        <th style=\"padding: 12px; text-align: center; border: 1px solid #ddd;\">Verdict</th>\n",
    "                        <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Progress Bar</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "        \"\"\"\n",
    "        \n",
    "        toxic_detected = []\n",
    "        for label, prob in predictions.items():\n",
    "            is_toxic = prob > 0.5\n",
    "            row_color = '#ffe6e6' if is_toxic else '#e6ffe6'\n",
    "            verdict = f'<span style=\"color: #c0392b; font-weight: bold;\">✓ TOXIC</span>' if is_toxic else '<span style=\"color: #27ae60;\">✓ Clean</span>'\n",
    "            \n",
    "            # Progress bar\n",
    "            bar_color = '#e74c3c' if is_toxic else '#2ecc71'\n",
    "            bar_width = int(prob * 100)\n",
    "            progress_bar = f\"\"\"\n",
    "                <div style=\"background-color: #ecf0f1; border-radius: 10px; overflow: hidden; width: 200px;\">\n",
    "                    <div style=\"background-color: {bar_color}; width: {bar_width}%; height: 20px; text-align: center; line-height: 20px; color: white; font-size: 11px; font-weight: bold;\">\n",
    "                        {prob:.1%}\n",
    "                    </div>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            if is_toxic:\n",
    "                toxic_detected.append(f\"{label} ({prob:.1%})\")\n",
    "            \n",
    "            html_output += f\"\"\"\n",
    "                <tr style=\"background-color: {row_color};\">\n",
    "                    <td style=\"padding: 10px; border: 1px solid #ddd; font-weight: bold;\">{label}</td>\n",
    "                    <td style=\"padding: 10px; border: 1px solid #ddd; text-align: center; font-family: monospace;\">{prob:.4f}</td>\n",
    "                    <td style=\"padding: 10px; border: 1px solid #ddd; text-align: center;\">{verdict}</td>\n",
    "                    <td style=\"padding: 10px; border: 1px solid #ddd;\">{progress_bar}</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_output += \"\"\"\n",
    "                </tbody>\n",
    "            </table>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Final verdict\n",
    "        if toxic_detected:\n",
    "            verdict_icon = \"🚫\"\n",
    "            verdict_text = \"TOXIC CONTENT DETECTED\"\n",
    "            verdict_color = \"#c0392b\"\n",
    "            verdict_bg = \"#ffe6e6\"\n",
    "            details = f\"Detected: {', '.join(toxic_detected)}\"\n",
    "        else:\n",
    "            verdict_icon = \"✅\"\n",
    "            verdict_text = \"CLEAN CONTENT\"\n",
    "            verdict_color = \"#27ae60\"\n",
    "            verdict_bg = \"#e6ffe6\"\n",
    "            details = \"No toxic content detected in this comment\"\n",
    "        \n",
    "        html_output += f\"\"\"\n",
    "            <div style=\"background-color: {verdict_bg}; padding: 20px; border-radius: 5px; margin-top: 20px; border-left: 5px solid {verdict_color};\">\n",
    "                <h3 style=\"color: {verdict_color}; margin-top: 0;\">\n",
    "                    {verdict_icon} {verdict_text}\n",
    "                </h3>\n",
    "                <p style=\"color: #555; margin: 5px 0 0 0;\">{details}</p>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        display(HTML(html_output))\n",
    "\n",
    "def on_clear_click(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "    text_input.value = ''\n",
    "\n",
    "def on_sample_click(b):\n",
    "    text_input.value = b.sample_text\n",
    "    on_predict_click(None)\n",
    "\n",
    "# Attach event handlers\n",
    "predict_button.on_click(on_predict_click)\n",
    "clear_button.on_click(on_clear_click)\n",
    "for btn in sample_buttons:\n",
    "    btn.on_click(on_sample_click)\n",
    "\n",
    "# Display UI\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; margin-bottom: 20px; box-shadow: 0 10px 30px rgba(0,0,0,0.3);\">\n",
    "    <h1 style=\"color: white; text-align: center; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\">\n",
    "        🛡️ Toxic Comment Classifier\n",
    "    </h1>\n",
    "    <p style=\"color: #e0e0e0; text-align: center; margin: 10px 0 0 0; font-size: 16px;\">\n",
    "        TF-IDF + Logistic Regression | Real-time Analysis\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML('<h3 style=\"color: #2c3e50; margin-bottom: 10px;\">💬 Enter Your Comment:</h3>'),\n",
    "    text_input,\n",
    "    widgets.HBox([predict_button, clear_button]),\n",
    "    widgets.HTML('<h4 style=\"color: #34495e; margin-top: 20px; margin-bottom: 10px;\">📌 Quick Samples:</h4>'),\n",
    "    widgets.HBox(sample_buttons),\n",
    "    widgets.HTML('<h4 style=\"color: #34495e; margin-top: 20px; margin-bottom: 10px;\">📈 Results:</h4>'),\n",
    "    output_area\n",
    "]))\n",
    "\n",
    "print(\"✓ Interactive UI loaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
