"""
Module 2: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cho Deep Learning
- L√†m s·∫°ch vƒÉn b·∫£n (lower-case, b·ªè URL/mention/emoji)
- Chu·∫©n h√≥a slang d·ª±a tr√™n slang.csv
- Tokenization v√† padding cho Keras
"""

import re
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
import nltk
from nltk.corpus import stopwords

# T·∫£i stopwords n·∫øu ch∆∞a c√≥
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)


class TextPreprocessor:
    """
    B·ªô ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n cho m√¥ h√¨nh Deep Learning
    """
    
    def __init__(self, slang_dict_path: str = None, remove_stopwords: bool = False):
        """
        Args:
            slang_dict_path: ƒê∆∞·ªùng d·∫´n t·ªõi slang.csv
            remove_stopwords: C√≥ lo·∫°i b·ªè stopwords kh√¥ng (m·∫∑c ƒë·ªãnh False v√¨ DL c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c)
        """
        self.slang_dict = {}
        self.remove_stopwords = remove_stopwords
        self.stop_words = set(stopwords.words('english')) if remove_stopwords else set()
        
        if slang_dict_path:
            self.load_slang_dict(slang_dict_path)
    
    def load_slang_dict(self, path: str):
        """Load t·ª´ ƒëi·ªÉn slang t·ª´ CSV"""
        try:
            df = pd.read_csv(path, encoding='utf-8')
            # Gi·∫£ ƒë·ªãnh c√≥ c·ªôt 'slang' v√† 'normalized' ho·∫∑c t∆∞∆°ng t·ª±
            if 'slang' in df.columns:
                for _, row in df.iterrows():
                    slang = str(row.get('slang', '')).lower().strip()
                    # N·∫øu c√≥ c·ªôt normalized, d√πng; kh√¥ng th√¨ ƒë·ªÉ tr·ªëng
                    normalized = str(row.get('normalized', slang)).lower().strip()
                    if slang:
                        self.slang_dict[slang] = normalized
            print(f"‚úì Loaded {len(self.slang_dict)} slang terms")
        except Exception as e:
            print(f"‚ö† Could not load slang dict: {e}")
    
    def clean_text(self, text: str) -> str:
        """
        L√†m s·∫°ch vƒÉn b·∫£n c∆° b·∫£n
        - Lower-case
        - B·ªè URLs
        - B·ªè mentions (@user)
        - B·ªè emoji/special chars (gi·ªØ d·∫•u c√¢u c∆° b·∫£n)
        - Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        """
        if not isinstance(text, str):
            return ""
        
        # Lower-case
        text = text.lower()
        
        # B·ªè URLs
        text = re.sub(r'http\S+|www\.\S+', '', text)
        
        # B·ªè mentions
        text = re.sub(r'@\w+', '', text)
        
        # B·ªè emoji v√† k√Ω t·ª± ƒë·∫∑c bi·ªát (gi·ªØ ch·ªØ, s·ªë, d·∫•u c√¢u c∆° b·∫£n)
        text = re.sub(r'[^\w\s\.\,\!\?\-\']', ' ', text)
        
        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def normalize_slang(self, text: str) -> str:
        """
        Thay th·∫ø slang b·∫±ng t·ª´ chu·∫©n h√≥a
        """
        if not self.slang_dict:
            return text
        
        words = text.split()
        normalized_words = []
        
        for word in words:
            # Ki·ªÉm tra slang dict
            normalized = self.slang_dict.get(word, word)
            
            # Lo·∫°i stopwords n·∫øu c·∫ßn
            if self.remove_stopwords and normalized in self.stop_words:
                continue
            
            normalized_words.append(normalized)
        
        return ' '.join(normalized_words)
    
    def preprocess(self, text: str) -> str:
        """
        Pipeline ti·ªÅn x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß
        """
        text = self.clean_text(text)
        text = self.normalize_slang(text)
        return text
    
    def preprocess_batch(self, texts: List[str]) -> List[str]:
        """
        Ti·ªÅn x·ª≠ l√Ω h√†ng lo·∫°t
        """
        return [self.preprocess(text) for text in texts]


def load_and_split_data(
    train_csv_path: str,
    test_size: float = 0.2,
    val_size: float = 0.1,
    random_state: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Load d·ªØ li·ªáu t·ª´ train.csv v√† chia th√†nh train/val/test
    
    Args:
        train_csv_path: ƒê∆∞·ªùng d·∫´n t·ªõi train.csv
        test_size: T·ª∑ l·ªá test (0.2 = 20%)
        val_size: T·ª∑ l·ªá validation (0.1 = 10% c·ªßa ph·∫ßn c√≤n l·∫°i sau test)
        random_state: Random seed
    
    Returns:
        (train_df, val_df, test_df)
    """
    from sklearn.model_selection import train_test_split
    
    # Load data
    df = pd.read_csv(train_csv_path)
    
    # Gi·∫£ ƒë·ªãnh c·ªôt vƒÉn b·∫£n l√† 'comment_text' v√† 6 nh√£n
    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
    
    # Ki·ªÉm tra c·ªôt
    assert 'comment_text' in df.columns, "Missing 'comment_text' column"
    for col in label_cols:
        assert col in df.columns, f"Missing label column: {col}"
    
    # Chia train/test tr∆∞·ªõc
    train_val_df, test_df = train_test_split(
        df, 
        test_size=test_size, 
        random_state=random_state,
        stratify=None  # B·ªè stratify ƒë·ªÉ tr√°nh l·ªói khi c√≥ combination nh√£n hi·∫øm
    )
    
    # Chia train/val
    train_df, val_df = train_test_split(
        train_val_df,
        test_size=val_size,
        random_state=random_state,
        stratify=None  # B·ªè stratify ƒë·ªÉ tr√°nh l·ªói khi c√≥ combination nh√£n hi·∫øm
    )
    
    print(f"‚úì Data split:")
    print(f"  - Train: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)")
    print(f"  - Val:   {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)")
    print(f"  - Test:  {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)")
    
    return train_df, val_df, test_df


def prepare_sequences(
    texts: List[str],
    tokenizer,
    max_len: int = 250
) -> np.ndarray:
    """
    Chuy·ªÉn vƒÉn b·∫£n th√†nh sequences v√† padding
    
    Args:
        texts: List vƒÉn b·∫£n
        tokenizer: Keras Tokenizer ƒë√£ fit
        max_len: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa sequence
    
    Returns:
        Padded sequences (numpy array)
    """
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')
    
    return padded


if __name__ == "__main__":
    # Test preprocess
    preprocessor = TextPreprocessor(
        slang_dict_path="../Data/slang.csv",
        remove_stopwords=False
    )
    
    sample_texts = [
        "OMG this is so fking toxic!!! @user http://spam.com üò°",
        "You're such an idiot lol",
        "This is a normal comment."
    ]
    
    print("\n=== Text Preprocessing Test ===")
    for text in sample_texts:
        cleaned = preprocessor.preprocess(text)
        print(f"Original: {text}")
        print(f"Cleaned:  {cleaned}\n")
