"""
Module 2: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cho Deep Learning (ENHANCED)
- L√†m s·∫°ch vƒÉn b·∫£n (lower-case, b·ªè URL/mention/emoji)
- Chu·∫©n h√≥a slang d·ª±a tr√™n slang.csv
- **ROBUST PROFANITY NORMALIZATION**: X·ª≠ l√Ω obfuscated profanity (f*ck, sh1t, b!tch, f u c k)
- **CONTEXT-AWARE**: "fucking good" ‚Üí "very good" (kh√¥ng toxic)
- **LEET SPEAK**: @ ‚Üí a, 1 ‚Üí i, 0 ‚Üí o
- **REPEATED CHARS**: "shiiiit" ‚Üí "shit", "fuuuuck" ‚Üí "fuck"
- Tokenization v√† padding cho Keras
"""

import re
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split

# Import patterns from centralized module
from .utils import (
    PROFANITY_OBFUSCATION_MAPPINGS as PROFANITY_PATTERNS,
    CHAT_LINGO_MAPPINGS as CHAT_MAP,
    EMOJI_SENTIMENT,
    NEGATION_PATTERNS,
    POSITIVE_WORDS,
    NEGATIVE_WORDS,
    POSITIVE_CONTEXTS,
    BENIGN_PROFANITY_PATTERN,
    INTENSIFIED_PATTERN,
    KILLER_SKILL_PATTERN,
    DAMN_POS_PATTERN,
    DAMN_CHAIN_PATTERN,
    LABEL_COLS,
    URL_PATTERN,
    MENTION_PATTERN,
    DEFAULT_CHAR_VOCAB
)

# Optional: Import spell checker (graceful fallback if not installed)
try:
    from autocorrect import Speller
    SPELL_CHECKER_AVAILABLE = True
except ImportError:
    SPELL_CHECKER_AVAILABLE = False
    Speller = None

# T·∫£i stopwords n·∫øu ch∆∞a c√≥
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)


# ========================================
# HELPER FUNCTIONS
# ========================================

def is_benign_profanity(text: str) -> bool:
    """
    Check if profanity appears in positive/benign context.
    Returns True if text contains benign profanity usage.
    
    Examples:
        - "This is badass music!" ‚Üí True
        - "Holy shit, that's amazing!" ‚Üí True
        - "Fuck you" ‚Üí False
    """
    text_lower = text.lower()
    for profanity, positive_words in POSITIVE_CONTEXTS.items():
        if profanity in text_lower:
            # Check if any positive context word appears near profanity
            if any(pos in text_lower for pos in positive_words):
                return True
    return False


class TextPreprocessor:
    """
    B·ªô ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n N√ÇNG CAP cho m√¥ h√¨nh Deep Learning
    
    T√≠nh nƒÉng:
    - Profanity normalization: f*ck ‚Üí fuck, sh1t ‚Üí shit
    - Context-aware: "fucking good" ‚Üí "very good" (benign)
    - Leet speak: @ ‚Üí a, 1 ‚Üí i, 0 ‚Üí o
    - Repeated characters: shiiiit ‚Üí shit
    - Chat lingo: u ‚Üí you, ur ‚Üí your
    """
    
    def __init__(self, slang_dict_path: str = None, remove_stopwords: bool = False, enable_spell_correction: bool = False):
        """
        Args:
            slang_dict_path: ƒê∆∞·ªùng d·∫´n t·ªõi slang.csv
            remove_stopwords: C√≥ lo·∫°i b·ªè stopwords kh√¥ng (m·∫∑c ƒë·ªãnh False v√¨ DL c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c)
            enable_spell_correction: B·∫≠t spell correction (c·∫ßn c√†i autocorrect). M·∫∑c ƒë·ªãnh T·∫ÆT cho b√†i to√°n toxic detection.
        """
        self.slang_dict = {}
        self.remove_stopwords = remove_stopwords
        self.stop_words = set(stopwords.words('english')) if remove_stopwords else set()
        self.enable_spell_correction = enable_spell_correction and SPELL_CHECKER_AVAILABLE
        
        # Initialize spell checker
        if self.enable_spell_correction:
            try:
                self.spell_checker = Speller(lang='en', fast=True)
            except Exception as e:
                print(f"Warning: Could not initialize spell checker: {e}")
                self.enable_spell_correction = False
                self.spell_checker = None
        else:
            self.spell_checker = None
        
        if slang_dict_path:
            self.load_slang_dict(slang_dict_path)
    
    def load_slang_dict(self, path: str):
        """Load t·ª´ ƒëi·ªÉn slang t·ª´ CSV"""
        try:
            df = pd.read_csv(path, encoding='utf-8')
            # Gi·∫£ ƒë·ªãnh c√≥ c·ªôt 'slang' v√† 'normalized' ho·∫∑c t∆∞∆°ng t·ª±
            if 'slang' in df.columns:
                for _, row in df.iterrows():
                    slang = str(row.get('slang', '')).lower().strip()
                    # N·∫øu c√≥ c·ªôt normalized, d√πng; kh√¥ng th√¨ ƒë·ªÉ tr·ªëng
                    normalized = str(row.get('normalized', slang)).lower().strip()
                    if slang:
                        self.slang_dict[slang] = normalized
            print(f"‚úì Loaded {len(self.slang_dict)} slang terms")
        except Exception as e:
            print(f"‚ö† Could not load slang dict: {e}")
    
    def normalize_profanity_context_aware(self, text: str) -> str:
        """
        Normalize profanity theo context - IMPROVED with NEGATIVE_WORDS check
        
        Logic:
        1. N·∫øu c√≥ NEGATIVE_WORDS (dead, kill, sucks, hate) ‚Üí KH√îNG normalize (gi·ªØ toxic)
           EXCEPTION: Kill idioms (kill it, killer at, killing it) ‚Üí LU√îN normalize (skill context)
        2. N·∫øu c√≥ POSITIVE_WORDS (good, amazing, awesome) ‚Üí normalize (benign)
        3. Kill idioms ‚Üí LU√îN normalize (lu√¥n l√† skill/performance context)
        
        ‚ö†Ô∏è LIMITATIONS (Heuristic-based approach):
        - False positives tr√™n idioms (~2-5%):
          ‚Ä¢ "dead tired" ‚Üí blocked (should normalize, but doesn't)
          ‚Ä¢ "dead serious" ‚Üí blocked (should normalize, but doesn't)
          ‚Ä¢ "trash talk" (gaming) ‚Üí blocked (may be benign)
        - Trade-off: Conservative approach (safer for toxic detection)
        - Deep Learning model s·∫Ω h·ªçc context t·ª´ training data ‚Üí compensate heuristic limits
        
        V√≠ d·ª• ƒê√öNG:
        - "fucking good" ‚Üí "very good" ‚úÖ (benign)
        - "fucking amazing" ‚Üí "very amazing" ‚úÖ (benign)
        - "fucking dead" ‚Üí "fucking dead" ‚ùå (TOXIC - c√≥ "dead")
        - "fucking sucks" ‚Üí "fucking sucks" ‚ùå (TOXIC - c√≥ "sucks")
        - "fuck you" ‚Üí "fuck you" ‚ùå (TOXIC - kh√¥ng c√≥ positive context)
        - "killer at chess" ‚Üí "expert at chess" ‚úÖ (skill context, ALWAYS benign)
        - "killing it" ‚Üí "doing great" ‚úÖ (performance context, ALWAYS benign)
        
        V√≠ d·ª• FALSE POSITIVE (acceptable):
        - "fucking dead tired" ‚Üí "fucking dead tired" ‚ùå (blocked, nh∆∞ng c√≥ th·ªÉ benign)
          ‚Üí Model s·∫Ω h·ªçc t·ª´ training data
        """
        text_lower = text.lower()
        
        # SPECIAL CASE 1: Kill idioms ALWAYS benign (skill/performance context)
        # These should ALWAYS be normalized, regardless of other words
        has_kill_idiom = (
            KILLER_SKILL_PATTERN.search(text_lower) or
            re.search(r'\bkilling\s+it\b', text_lower) or
            re.search(r'\bkill\s+it\b', text_lower) or
            re.search(r'\bkill\s+the\s+game\b', text_lower)
        )
        
        if not has_kill_idiom:
            # Check if text contains NEGATIVE_WORDS ‚Üí DO NOT normalize (keep toxic)
            has_negative = any(neg in text_lower for neg in NEGATIVE_WORDS)
            if has_negative:
                return text  # Keep profanity as-is (genuinely toxic)
            
            # Check if text contains POSITIVE_WORDS ‚Üí normalize (benign profanity)
            has_positive = any(pos in text_lower for pos in POSITIVE_WORDS)
            if not has_positive:
                return text  # No positive context, keep profanity
        
        # Has positive context, proceed with normalization
        # Handle intensified patterns first
        text = INTENSIFIED_PATTERN.sub(lambda m: f"{m.group(1)} very {m.group(3)}", text)
        
        # Handle benign profanity
        text = BENIGN_PROFANITY_PATTERN.sub(lambda m: f"very {m.group(2)}", text)
        
        # Map damn + positive context to a neutral intensifier
        text = DAMN_CHAIN_PATTERN.sub(lambda m: f"very {m.group(2)}", text)
        text = DAMN_POS_PATTERN.sub(lambda m: f"very {m.group(2)}", text)

        # Map "killer at/in/on" to "expert at/in/on" (positive skill context)
        text = KILLER_SKILL_PATTERN.sub(lambda m: f"expert {m.group(1)}", text)
        
        # Handle "killing it" / "kill it" / "kill the game" (positive performance context)
        text = re.sub(r'\bkilling\s+it\b', 'doing great', text, flags=re.IGNORECASE)
        text = re.sub(r'\bkill\s+it\b', 'dominating', text, flags=re.IGNORECASE)
        text = re.sub(r'\bkill\s+the\s+game\b', 'dominating', text, flags=re.IGNORECASE)
        
        return text
    
    def normalize_obfuscated_profanity(self, text: str) -> str:
        """
        Normalize obfuscated profanity
        
        V√≠ d·ª•:
        - "f u c k" ‚Üí "fuck"
        - "sh*t" ‚Üí "shit"
        - "b!tch" ‚Üí "bitch"
        - "f_u_c_k" ‚Üí "fuck"
        """
        for pattern, replacement in PROFANITY_PATTERNS:
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        return text
    
    def handle_kys_context(self, text: str) -> str:
        """
        KYS (kill yourself) is ALWAYS toxic, regardless of context.
        No special handling needed - will be normalized via chat_lingo.
        
        Note: Even in gaming context ("kys in video game"), it's still a harmful phrase
        that should be flagged. The model will learn from labeled data.
        """
        return text
    
    def normalize_leet_speak(self, text: str) -> str:
        """
        Normalize leet speak (1337 speak)
        
        V√≠ d·ª•:
        - "idi0t" ‚Üí "idiot"
        - "st*pid" ‚Üí "stupid"
        - "@sshole" ‚Üí "asshole"
        - "sh1t" ‚Üí "shit"
        """
        # Common leet speak mappings
        text = re.sub(r'@', 'a', text)
        text = re.sub(r'1', 'i', text)
        text = re.sub(r'3', 'e', text)
        text = re.sub(r'0', 'o', text)
        text = re.sub(r'5', 's', text)
        text = re.sub(r'7', 't', text)
        text = re.sub(r'\$', 's', text)
        
        return text
    
    def collapse_repeated_chars(self, text: str) -> str:
        """
        Collapse repeated characters (IMPROVED - more aggressive)
        
        V√≠ d·ª•:
        - "shiiiit" ‚Üí "shit" (aggressive: max 1 repeat for i,o,u,a,e)
        - "fuuuuuck" ‚Üí "fuck"
        - "hahahaha" ‚Üí "haha"
        - "loool" ‚Üí "lol"
        - "yessss" ‚Üí "yes"
        """
        # For vowels and common repeated chars, collapse to single
        # Pattern: 3+ same chars ‚Üí 1 char for vowels
        for vowel in ['a', 'e', 'i', 'o', 'u', 'y']:
            # Match 3 or more of same vowel
            text = re.sub(f'{vowel}{{3,}}', vowel, text, flags=re.IGNORECASE)
        
        # For consonants, allow max 2 (for words like "happy", "litter")
        # Pattern: 3+ same chars ‚Üí 2 chars for consonants
        text = re.sub(r'(.)\1{2,}', r'\1\1', text)
        
        return text
    
    def normalize_chat_lingo(self, text: str) -> str:
        """
        Normalize chat lingo
        
        V√≠ d·ª•:
        - "u" ‚Üí "you"
        - "ur" ‚Üí "your"
        - "r" ‚Üí "are"
        - "wtf" ‚Üí "what the fuck"
        - "kys" ‚Üí "kill yourself" (ALWAYS toxic, no exceptions)
        """
        # CHAT_MAP is a tuple of (pattern, replacement) tuples
        for pattern, replacement in CHAT_MAP:
            text = re.sub(pattern, replacement, text)
        
        return text
    
    def collapse_punctuation(self, text: str) -> str:
        """
        Collapse repeated punctuation
        
        V√≠ d·ª•:
        - "!!!" ‚Üí "!"
        - "???" ‚Üí "?"
        - "..." ‚Üí "."
        """
        text = re.sub(r'!{2,}', '!', text)
        text = re.sub(r'\?{2,}', '?', text)
        text = re.sub(r'\.{2,}', '.', text)
        return text
    
    def clean_text(self, text: str) -> str:
        """
        L√†m s·∫°ch vƒÉn b·∫£n C∆† B·∫¢N (tr∆∞·ªõc khi normalize profanity)
        - Lower-case
        - B·ªè URLs
        - B·ªè mentions (@user)
        - B·ªè HTML tags
        - Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        """
        if not isinstance(text, str):
            return ""
        
        # Lower-case
        text = text.lower()
        
        # B·ªè HTML tags
        text = re.sub(r'<[^>]+>', ' ', text)
        
        # B·ªè URLs
        text = re.sub(r'http\S+|www\.\S+', '', text)
        
        # B·ªè mentions
        text = re.sub(r'@\w+', '', text)
        
        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng (tr∆∞·ªõc khi x·ª≠ l√Ω profanity)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def normalize_slang(self, text: str) -> str:
        """
        Thay th·∫ø slang b·∫±ng t·ª´ chu·∫©n h√≥a
        """
        if not self.slang_dict:
            return text
        
        words = text.split()
        normalized_words = []
        
        for word in words:
            # Ki·ªÉm tra slang dict
            normalized = self.slang_dict.get(word, word)
            
            # Lo·∫°i stopwords n·∫øu c·∫ßn
            if self.remove_stopwords and normalized in self.stop_words:
                continue
            
            normalized_words.append(normalized)
        
        return ' '.join(normalized_words)
    
    def preserve_emoji_sentiment(self, text: str) -> str:
        """
        Replace emojis with sentiment words before removal.
        Helps preserve emotional context from emojis.
        
        Examples:
        - "üò† angry post" ‚Üí "angry angry post"
        - "great news üòÄ" ‚Üí "great news happy"
        - "I hate this üëé" ‚Üí "I hate this thumbs down"
        """
        for emoji, sentiment in EMOJI_SENTIMENT.items():
            if emoji in text:
                text = text.replace(emoji, f" {sentiment} ")
        return text
    
    def normalize_negations(self, text: str) -> str:
        """
        Handle advanced negation patterns.
        
        Examples:
        - "not bad" ‚Üí "good"
        - "not good" ‚Üí "bad"
        - "don't like" ‚Üí "dislike"
        """
        for pattern, replacement in NEGATION_PATTERNS:
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        return text
    
    def spell_correct(self, text: str) -> str:
        """
        Apply spell correction to fix typos.
        Only runs if autocorrect is installed and enabled.
        
        Examples:
        - "teh" ‚Üí "the"
        - "recieve" ‚Üí "receive"
        - "definately" ‚Üí "definitely"
        """
        if not self.enable_spell_correction or not self.spell_checker:
            return text
        
        try:
            # Split and correct word by word
            words = text.split()
            corrected_words = []
            
            for word in words:
                # Skip if word is too short or contains numbers
                if len(word) <= 2 or any(c.isdigit() for c in word):
                    corrected_words.append(word)
                    continue
                
                # Skip profanity and toxic words (they're intentional)
                if word.lower() in ['fuck', 'shit', 'bitch', 'ass', 'damn', 'hell']:
                    corrected_words.append(word)
                    continue
                
                # Apply spell correction
                corrected = self.spell_checker(word)
                corrected_words.append(corrected)
            
            return ' '.join(corrected_words)
        except Exception as e:
            # Fail gracefully
            return text
    
    def normalize_numbers(self, text: str) -> str:
        """
        Normalize numbers to reduce vocabulary size.
        
        Examples:
        - "123" ‚Üí "<NUM>"
        - "$50" ‚Üí "<NUM> dollars"
        - "25%" ‚Üí "<NUM> percent"
        """
        # Replace percentages
        text = re.sub(r'(\d+)%', r'<NUM> percent', text)
        
        # Replace currency
        text = re.sub(r'\$(\d+(?:\.\d+)?)', r'<NUM> dollars', text)
        text = re.sub(r'¬£(\d+(?:\.\d+)?)', r'<NUM> pounds', text)
        text = re.sub(r'‚Ç¨(\d+(?:\.\d+)?)', r'<NUM> euros', text)
        
        # Replace standalone numbers (but keep single digits for context)
        text = re.sub(r'\b\d{2,}\b', '<NUM>', text)
        
        return text
    
    def preprocess(self, text: str) -> str:
        """
        Pipeline ti·ªÅn x·ª≠ l√Ω ENHANCED - Th·ª© t·ª± quan tr·ªçng!
        
        1. Clean basic (lowercase, remove URLs, mentions, HTML)
        2. Preserve emoji sentiment (emoji ‚Üí sentiment words)
        3. Normalize numbers FIRST (before leet speak converts digits)
        4. Context-aware profanity normalization ("fucking good" ‚Üí "very good")
        5. Advanced negation handling ("not bad" ‚Üí "good")
        6. Collapse repeated chars - AGGRESSIVE ("shiiiit" ‚Üí "shit")
        7. Collapse punctuation ("!!!" ‚Üí "!")
        8. Normalize leet speak ("@sshole" ‚Üí "asshole", "sh1t" ‚Üí "shit")
        9. Normalize obfuscated profanity ("f u c k" ‚Üí "fuck", "sh*t" ‚Üí "shit")
        10. Normalize chat lingo ("u" ‚Üí "you", "wtf" ‚Üí "what the fuck") - EXPANDED
        11. Normalize slang t·ª´ dictionary
        12. Remove emoji/special chars (gi·ªØ ch·ªØ, s·ªë, d·∫•u c√¢u)
        13. Final whitespace normalization
        """
        # Step 1: Basic cleaning
        text = self.clean_text(text)
        
        # Step 2: Preserve emoji sentiment (BEFORE removal)
        text = self.preserve_emoji_sentiment(text)
        
        # Step 3: Normalize numbers FIRST (before leet speak converts digits)
        text = self.normalize_numbers(text)
        
        # Step 4: Context-aware profanity (TR∆Ø·ªöC khi normalize obfuscated)
        text = self.normalize_profanity_context_aware(text)
        
        # Step 5: Advanced negation handling
        text = self.normalize_negations(text)
        
        # Step 6: Collapse repeated chars - AGGRESSIVE
        text = self.collapse_repeated_chars(text)
        
        # Step 7: Collapse punctuation
        text = self.collapse_punctuation(text)
        
        # Step 8: Normalize leet speak
        text = self.normalize_leet_speak(text)
        
        # Step 9: Normalize obfuscated profanity
        text = self.normalize_obfuscated_profanity(text)
        
        # Step 9.5: Handle "kys" with context awareness (BEFORE chat lingo)
        text = self.handle_kys_context(text)
        
        # Step 10: Normalize chat lingo - EXPANDED
        text = self.normalize_chat_lingo(text)
        
        # Step 11: Normalize slang
        text = self.normalize_slang(text)
        
        # Step 12: Remove remaining emoji and special chars
        text = re.sub(r'[^\w\s\.\,\!\?\-\']', ' ', text)
        
        # Step 13: Final whitespace normalization
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def preprocess_batch(self, texts: List[str]) -> List[str]:
        """
        Ti·ªÅn x·ª≠ l√Ω h√†ng lo·∫°t
        """
        return [self.preprocess(text) for text in texts]


def load_and_split_data(
    train_csv_path: str,
    test_size: float = 0.2,
    val_size: float = 0.1,
    random_state: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Load d·ªØ li·ªáu t·ª´ train.csv v√† chia th√†nh train/val/test
    
    Args:
        train_csv_path: ƒê∆∞·ªùng d·∫´n t·ªõi train.csv
        test_size: T·ª∑ l·ªá test (0.2 = 20%)
        val_size: T·ª∑ l·ªá validation (0.1 = 10% c·ªßa ph·∫ßn c√≤n l·∫°i sau test)
        random_state: Random seed
    
    Returns:
        (train_df, val_df, test_df)
    """
    # Load data
    df = pd.read_csv(train_csv_path)
    
    # Ki·ªÉm tra c·ªôt
    assert 'comment_text' in df.columns, "Missing 'comment_text' column"
    for col in LABEL_COLS:
        assert col in df.columns, f"Missing label column: {col}"
    
    # Chia train/test tr∆∞·ªõc
    train_val_df, test_df = train_test_split(
        df, 
        test_size=test_size, 
        random_state=random_state,
        stratify=None  # B·ªè stratify ƒë·ªÉ tr√°nh l·ªói khi c√≥ combination nh√£n hi·∫øm
    )
    
    # Chia train/val
    train_df, val_df = train_test_split(
        train_val_df,
        test_size=val_size,
        random_state=random_state,
        stratify=None  # B·ªè stratify ƒë·ªÉ tr√°nh l·ªói khi c√≥ combination nh√£n hi·∫øm
    )
    
    print(f"‚úì Data split:")
    print(f"  - Train: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)")
    print(f"  - Val:   {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)")
    print(f"  - Test:  {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)")
    
    return train_df, val_df, test_df


def prepare_sequences(
    texts: List[str],
    tokenizer,
    max_len: int = 250
) -> np.ndarray:
    """
    Chuy·ªÉn vƒÉn b·∫£n th√†nh sequences v√† padding
    
    Args:
        texts: List vƒÉn b·∫£n
        tokenizer: Keras Tokenizer ƒë√£ fit
        max_len: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa sequence
    
    Returns:
        Padded sequences (numpy array)
    """
    try:
        from tensorflow.keras.preprocessing.sequence import pad_sequences
    except ImportError:
        from keras.preprocessing.sequence import pad_sequences
    
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')
    
    return padded


# ============================
# Character-level tokenization
# ============================

def get_default_char_vocab() -> List[str]:
    """
    Vocab k√Ω t·ª± m·∫∑c ƒë·ªãnh sau khi ƒë√£ qua b∆∞·ªõc normalize/clean:
    - Ch·ªâ c√≤n ch·ªØ c√°i th∆∞·ªùng a-z, ch·ªØ s·ªë, kho·∫£ng tr·∫Øng v√† m·ªôt s·ªë d·∫•u c√¢u c∆° b·∫£n
    - Gi·ªØ d·∫•u g·∫°ch d∆∞·ªõi v√† nh√°y ƒë∆°n v√¨ v·∫´n c√≤n sau clean
    """
    return DEFAULT_CHAR_VOCAB


def prepare_char_sequences(
    texts: List[str],
    max_char_len: int = 400,
    char_vocab: List[str] = None
) -> np.ndarray:
    """
    Bi·∫øn vƒÉn b·∫£n th√†nh chu·ªói k√Ω t·ª± c·ªë ƒë·ªãnh ƒë·ªô d√†i cho Char-CNN/Char-Embedding
    - 0: PAD, 1: UNK, 2..: c√°c k√Ω t·ª± trong vocab
    """
    if char_vocab is None:
        char_vocab = get_default_char_vocab()
    ch2id = {ch: idx + 2 for idx, ch in enumerate(char_vocab)}  # 0:PAD, 1:UNK
    PAD = 0
    UNK = 1

    X = np.zeros((len(texts), max_char_len), dtype=np.int32)

    for i, t in enumerate(texts):
        # ƒë·∫£m b·∫£o string v√† lowercase ƒë√£ ƒë∆∞·ª£c clean tr∆∞·ªõc ƒë√≥
        if not isinstance(t, str):
            t = ""
        # c·∫Øt ho·∫∑c pad
        seq_ids = []
        for ch in t[:max_char_len]:
            seq_ids.append(ch2id.get(ch, UNK))
        # g√°n v√†o ma tr·∫≠n X (pad h·∫≠u)
        if seq_ids:
            X[i, :len(seq_ids)] = np.array(seq_ids, dtype=np.int32)

    return X


if __name__ == "__main__":
    # Test preprocess v·ªõi Enhanced Pipeline
    preprocessor = TextPreprocessor(
        slang_dict_path="../Data/slang.csv",
        remove_stopwords=False
    )
    
    # Test cases covering all normalization techniques
    sample_texts = [
        # Obfuscated profanity
        "f u c k this sh*t and b!tch",
        "You are such a f_u_c_k_i_n_g idiot",
        
        # Context-aware profanity
        "This is fucking good and amazing!",
        "So fucking awesome dude!",
        
        # Leet speak
        "You @re such an idi0t st*pid @sshole",
        "sh1t happens man",
        
        # Repeated chars
        "shiiiit this is sooooo baaaad",
        "fuuuuuck youuuuu",
        
        # Chat lingo
        "OMG u r so stupid wtf is wrong with u",
        "ur an idiot lol",
        
        # Mixed everything
        "f u c k this sh*t @user!!! u r such a f*cking idi0t omg wtf",
        
        # Benign (should NOT be toxic after normalization)
        "This is fucking awesome! Love it!!!",
        "So fucking good, really great performance",
        
        # Context: killer used positively
        "You're a killer at chess",
        "She is killer in math",
        
        # Slang/obfuscation acronyms
        "kys you loser",
        "k y s now",
        "stfu and leave",
        "gtfo from here",
        
        # Damn as positive intensifier
        "Damn brilliant idea",
        "that's damn amazing",
        
        # Normal comment
        "This is a normal comment.",
    ]
    
    print("\n" + "="*80)
    print("MODULE 2 - ENHANCED PREPROCESSING TEST")
    print("="*80)
    
    for i, text in enumerate(sample_texts, 1):
        cleaned = preprocessor.preprocess(text)
        print(f"\n{i}. Original:  {text}")
        print(f"   Cleaned:   {cleaned}")
        
        # Highlight key transformations
        if "f u c k" in text.lower():
            print(f"   ‚úì Obfuscated profanity normalized")
        if "fucking good" in text.lower() or "fucking awesome" in text.lower():
            print(f"   ‚úì Context-aware: benign profanity ‚Üí intensifier")
        if any(c in text for c in ['@', '0', '1', '3', '5', '7', '$']):
            print(f"   ‚úì Leet speak normalized")
        if re.search(r'(.)\1{3,}', text):
            print(f"   ‚úì Repeated characters collapsed")
        if re.search(r'\bkiller\s+(at|in|on)\b', text, flags=re.IGNORECASE):
            print(f"   ‚úì Context-aware: 'killer <prep>' ‚Üí 'expert <prep>'")
        if re.search(r'\b(kys|k[\W_]*y[\W_]*s)\b', text, flags=re.IGNORECASE):
            print(f"   ‚úì Slang: 'kys' ‚Üí 'kill yourself'")
        if re.search(r'\bst[\W_]*f[\W_]*u\b', text, flags=re.IGNORECASE):
            print(f"   ‚úì Slang: 'stfu' ‚Üí 'shut the fuck up'")
        if re.search(r'\bgt[\W_]*f[\W_]*o\b', text, flags=re.IGNORECASE):
            print(f"   ‚úì Slang: 'gtfo' ‚Üí 'get the fuck out'")
        if re.search(r'\b(damn|damned|dammit)\b', text, flags=re.IGNORECASE):
            print(f"   ‚úì Context-aware: 'damn' as intensifier ‚Üí 'very'")
    
    print("\n" + "="*80)
    print("SUMMARY:")
    print("‚úì Obfuscated profanity: f*ck, f u c k, b!tch ‚Üí normalized")
    print("‚úì Context-aware: 'fucking good' ‚Üí 'very good' (benign)")
    print("‚úì Leet speak: @, 0, 1 ‚Üí a, o, i")
    print("‚úì Repeated chars: shiiiit ‚Üí shiit")
    print("‚úì Chat lingo: u ‚Üí you, wtf ‚Üí what the fuck")
    print("="*80)
