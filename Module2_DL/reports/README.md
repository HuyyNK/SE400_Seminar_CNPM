# Module 2 Reports - README

ThÆ° má»¥c nÃ y chá»©a **3 bÃ¡o cÃ¡o toÃ n diá»‡n** vá» Module 2: Baseline Deep Learning Models.

---

## ğŸ“„ Danh sÃ¡ch BÃ¡o cÃ¡o

### 1. **MODULE2_COMPREHENSIVE_REPORT.md** ğŸ“Š
**BÃ¡o cÃ¡o Ä‘áº§y Ä‘á»§ vÃ  chi tiáº¿t nháº¥t**

**Ná»™i dung:**
- Tá»•ng quan Module 2 (má»¥c tiÃªu, dataset, yÃªu cáº§u Ä‘áº·c táº£)
- Kiáº¿n trÃºc CNN vÃ  BiLSTM chi tiáº¿t (layer-by-layer)
- QuÃ¡ trÃ¬nh training (11 epochs CNN, 14 epochs BiLSTM)
- Káº¿t quáº£ evaluation Ä‘áº§y Ä‘á»§ (AUC, F1, Precision, Recall)
- So sÃ¡nh CNN vs BiLSTM (5/6 labels BiLSTM tháº¯ng)
- PhÃ¢n tÃ­ch optimal thresholds
- Error analysis (false positives/negatives)
- Khuyáº¿n nghá»‹ deployment (BiLSTM recommended)
- Lessons learned & next steps

**Äá»™ dÃ i:** ~200 pages (náº¿u in ra)  
**Audience:** Technical team, researchers  
**Use case:** Deep dive vÃ o model performance

---

### 2. **EXECUTIVE_SUMMARY.md** ğŸ“‹
**BÃ¡o cÃ¡o tÃ³m táº¯t cho quáº£n lÃ½ vÃ  stakeholders**

**Ná»™i dung:**
- Káº¿t quáº£ chÃ­nh (CNN AUC 0.9796, BiLSTM AUC 0.9832)
- So sÃ¡nh trá»±c quan CNN vs BiLSTM
- Khuyáº¿n nghá»‹ production (BiLSTM vá»›i optimal thresholds)
- Key metrics at a glance
- Deliverables checklist (100% complete)
- Next steps (Module 3 preparation)

**Äá»™ dÃ i:** ~10 pages  
**Audience:** Managers, stakeholders  
**Use case:** Quick overview, decision making

---

### 3. **CNN_vs_BiLSTM_COMPARISON.md** ğŸ“ˆ
**BÃ¡o cÃ¡o so sÃ¡nh trá»±c quan vá»›i charts vÃ  biá»ƒu Ä‘á»“**

**Ná»™i dung:**
- Biá»ƒu Ä‘á»“ ASCII so sÃ¡nh hiá»‡u suáº¥t
- Per-label F1-score comparison (visual bars)
- Training efficiency comparison
- Precision vs Recall trade-offs
- Error analysis highlights
- Use case recommendations
- Final verdict (BiLSTM wins 5/6 labels)

**Äá»™ dÃ i:** ~15 pages  
**Audience:** Technical + non-technical  
**Use case:** Visual presentation, quick comparison

---

## ğŸ¯ CÃ¡ch Sá»­ dá»¥ng

### Náº¿u báº¡n cáº§n:

**1. Hiá»ƒu sÃ¢u vá» models:**
â†’ Äá»c `MODULE2_COMPREHENSIVE_REPORT.md`

**2. TrÃ¬nh bÃ y cho quáº£n lÃ½:**
â†’ Äá»c `EXECUTIVE_SUMMARY.md`

**3. So sÃ¡nh trá»±c quan CNN vs BiLSTM:**
â†’ Äá»c `CNN_vs_BiLSTM_COMPARISON.md`

**4. Táº¥t cáº£:**
â†’ Äá»c theo thá»© tá»±: Executive Summary â†’ Comparison â†’ Comprehensive

---

## ğŸ“Š Key Findings (TL;DR)

### Module 2 Status: âœ… **HOÃ€N THÃ€NH 100%**

**Models trained:**
- âœ… CNN: AUC 0.9796, F1 0.5607
- âœ… BiLSTM: AUC 0.9832, F1 0.5843

**Winner:** ğŸ† **BiLSTM**
- +4.2% F1 macro
- +24% identity_hate F1
- -22% validation loss
- Wins 5/6 labels

**Recommendation:** 
- **Production:** BiLSTM with optimal thresholds
- **High-volume:** Hybrid CNNâ†’BiLSTM pipeline
- **Speed-critical:** CNN only

**Next:** Module 3 - Transformer models (BERT/RoBERTa)

---

## ğŸ“ File Structure

```
Module2_DL/reports/
â”œâ”€â”€ README.md                           (this file)
â”œâ”€â”€ MODULE2_COMPREHENSIVE_REPORT.md     (detailed report)
â”œâ”€â”€ EXECUTIVE_SUMMARY.md                (management summary)
â””â”€â”€ CNN_vs_BiLSTM_COMPARISON.md         (visual comparison)
```

---

## ğŸ”— Related Files

**Models:**
```
Module2_DL/artifacts/
â”œâ”€â”€ toxic_cnn_model.h5              (61 MB)
â”œâ”€â”€ toxic_bilstm_model.h5           (59 MB)
â”œâ”€â”€ config_cnn.json
â”œâ”€â”€ config_bilstm.json
â”œâ”€â”€ tokenizer_cnn.json
â”œâ”€â”€ tokenizer_bilstm.json
â”œâ”€â”€ history_cnn.json
â”œâ”€â”€ history_bilstm.json
â”œâ”€â”€ report_baseline_dl_cnn.json     (evaluation metrics)
â”œâ”€â”€ report_baseline_dl_bilstm.json  (evaluation metrics)
â”œâ”€â”€ learning_curves_cnn.png
â”œâ”€â”€ learning_curves_bilstm.png
â”œâ”€â”€ pr_curves_cnn.png
â””â”€â”€ pr_curves_bilstm.png
```

**Code:**
```
Module2_DL/src/
â”œâ”€â”€ preprocess.py       (text preprocessing)
â”œâ”€â”€ models.py           (CNN & BiLSTM architectures)
â”œâ”€â”€ train.py            (training pipeline)
â”œâ”€â”€ evaluate.py         (evaluation with optimal thresholds)
â””â”€â”€ infer.py            (production inference)
```

---

## ğŸ“ˆ Performance Highlights

### Overall Metrics (Test Set)

| Metric | CNN | BiLSTM | Winner |
|--------|-----|--------|--------|
| **AUC** | 0.9796 | **0.9832** | BiLSTM (+0.36%) |
| **F1 Macro** | 0.5607 | **0.5843** | BiLSTM (+4.21%) |
| **F1 Micro** | 0.6831 | **0.7210** | BiLSTM (+5.55%) |

### Per-Label F1 (Test Set)

| Label | CNN | BiLSTM | Winner |
|-------|-----|--------|--------|
| toxic | 0.7861 | **0.8100** | BiLSTM (+3.0%) |
| severe_toxic | 0.4725 | **0.4963** | BiLSTM (+5.0%) |
| obscene | 0.8062 | **0.8117** | BiLSTM (+0.7%) |
| **threat** | **0.2258** | 0.2071 | CNN (+9.0%) |
| insult | 0.7075 | **0.7269** | BiLSTM (+2.7%) |
| identity_hate | 0.3659 | **0.4536** | BiLSTM (+24.0%) ğŸ¯ |

---

## âœ… Module 2 Checklist

**Requirements tá»« Ä‘áº·c táº£:**

- [x] Keras/TensorFlow implementation
- [x] Embedding layer (GloVe 300d, frozen)
- [x] Lá»±a chá»n A: Conv1D + GlobalMaxPooling âœ…
- [x] Lá»±a chá»n B: Bidirectional LSTM âœ…
- [x] Classification layers (Dense + Dropout)
- [x] 6 sigmoid outputs (multi-label)
- [x] binary_crossentropy loss + Adam optimizer
- [x] Validation set + EarlyStopping
- [x] Model files (.h5) exported
- [x] Performance reports (AUC, F1, Precision, Recall)

**Completion: 100%** âœ…

---

## ğŸ“ Key Learnings

### Technical:
- âœ… Multi-kernel CNN captures n-gram patterns well
- âœ… Bidirectional LSTM crucial for context (+5-10%)
- âœ… EarlyStopping prevents overfitting effectively
- âœ… Optimal thresholds improve rare class F1 by 10-20%

### Data Science:
- âš ï¸ Class imbalance is challenging (threat 0.3% only)
- âœ… Context understanding matters (BiLSTM +24% identity_hate)
- âš ï¸ Sarcasm detection still difficult
- âœ… Validation set essential

### Engineering:
- âœ… Clean modular code â†’ Easy to extend
- âœ… Comprehensive logging â†’ Reproducible
- âœ… Artifact management â†’ Production ready
- âœ… Evaluation framework â†’ Fair comparison

---

## ğŸš€ Next Steps

### Immediate:
1. âœ… Deploy BiLSTM model to production
2. âœ… Use optimal thresholds from report
3. âœ… Monitor inference performance

### Module 3 Preparation:
**Baseline to beat:**
- AUC: 0.9832 (BiLSTM)
- F1 macro: 0.5843
- Identity_hate F1: 0.4536

**Expected Transformer improvements:**
- AUC: +1-2% â†’ 0.985-0.995
- F1: +5-10% â†’ 0.61-0.64
- Identity_hate: +10-15% â†’ 0.50-0.55

**Trade-off:**
- Speed: 15ms â†’ 50-200ms (slower)
- Model size: 59MB â†’ 400-500MB (larger)

---

## ğŸ“ Contact

**Project:** SE405 - Seminar CNPM  
**Module:** 2 - Baseline Deep Learning Models  
**Date:** November 27, 2025  
**Status:** âœ… Complete

---

**Happy Reading! ğŸ“–**
